{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2899706",
   "metadata": {},
   "source": [
    "# High level view of the workflow\n",
    "* Get data ready (turn into tensors)\n",
    "* Build or pick a pretrained model which suits our problem\n",
    "    * Pick a loss function & optimizer\n",
    "    * Build a training loop\n",
    "* Fit the model to the data and make a prediction\n",
    "* Evaluate the model\n",
    "* Improve through experimentation\n",
    "* Save and reload the trained model\n",
    "\n",
    "Resources:\n",
    "* https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "67646cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.9.1+cu130', True)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn # all of PyTorch's neural network modules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.__version__, torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a4f6be",
   "metadata": {},
   "source": [
    "## 1. Data preparing and loading\n",
    "\n",
    "Can come in any form:\n",
    "* Spreadsheet\n",
    "* Images\n",
    "* Videos\n",
    "* Audio\n",
    "* DNA\n",
    "* Text\n",
    "\n",
    "Two parts:\n",
    "1. Get data into a numerical representation\n",
    "2. Build a model to learn patterns in that numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "425b5bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create some known data using linear regression\n",
    "\n",
    "# Known parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create features and labels\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start=start, end=end, step=step).unsqueeze(dim=1) # features\n",
    "y = weight * X + bias # labels\n",
    "\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f9d62e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "956034af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split data into training and test sets\n",
    "train_split = int(0.8 * len(X)) # 80% of data for training\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693b25c",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b4b4845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=None):\n",
    "    \"\"\"\n",
    "    Plots training data, test data and compares predictions to ground truth labels.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "\n",
    "    # Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "    # Plot predictions if they exist\n",
    "    if predictions is not None:\n",
    "        # Plot predictions in red (predictions were made on the test data)\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend(prop={\"size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8a23ee16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f3c0e",
   "metadata": {},
   "source": [
    "## 2. Build model\n",
    "\n",
    "* Start with random values of weight and bias\n",
    "* Look at training data and adjust the random values to get closer to the ideal values\n",
    "\n",
    "The process:\n",
    "1. Gradient descent\n",
    "2. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "20bae70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression model class\n",
    "class LinearRegressionModel(nn.Module): # almost every PyTorch model subclasses nn.Module, the base class for all neural network modules\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float)) # start with random weights\n",
    "        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float)) # start with random bias\n",
    "        # Note: requires_grad=True means PyTorch will track gradients with respect to these parameters during backpropagation\n",
    "\n",
    "    # Forward method to define the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # x is the input data\n",
    "        return self.weights * x + self.bias # linear regression formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d812f4",
   "metadata": {},
   "source": [
    "### PyTorch model building essentials\n",
    "\n",
    "* torch.nn: all of the building blocks for computational graphs (which covers neural networks)\n",
    "* torch.nn.Parameter: what parameters our model should try and learn, often a PyTorch layer from torch.nn sets these\n",
    "* torch.nn.Module: base class for all NN models. If subclassed, overwrite `forward`\n",
    "* torch.optim: where optimizers in PyTorch live; they help with gradient descent\n",
    "* def forward(): all nn.Module subclasses are required to overwrite this. It defines what happens in forward computation\n",
    "* torch.utils.data.Dataset: map between key (label) and sample (features) pairs of the data\n",
    "* torch.utils.data.DataLoader: creates Python iterable over a torch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfc13f",
   "metadata": {},
   "source": [
    "### Checking contents of PyTorch model\n",
    "\n",
    "Create an instance of the class and check contents.\n",
    "\n",
    "Check model parameters using `.parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8ccc66ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegressionModel(),\n",
       " Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a manual seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate the model\n",
    "model_0 = LinearRegressionModel()\n",
    "model_0, model_0.weights, model_0.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1b592715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the parameters\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c62d5d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List named parameters\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83a52c",
   "metadata": {},
   "source": [
    "### Making prediction using `torch.inference_mode()`\n",
    "\n",
    "Check how well it predicts `y_test` based on `x_test`\n",
    "\n",
    "The model passes our data through the `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e9053967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3982],\n",
       "        [0.4049],\n",
       "        [0.4116],\n",
       "        [0.4184],\n",
       "        [0.4251],\n",
       "        [0.4318],\n",
       "        [0.4386],\n",
       "        [0.4453],\n",
       "        [0.4520],\n",
       "        [0.4588]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model_0(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0a22b0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3982],\n",
       "        [0.4049],\n",
       "        [0.4116],\n",
       "        [0.4184],\n",
       "        [0.4251],\n",
       "        [0.4318],\n",
       "        [0.4386],\n",
       "        [0.4453],\n",
       "        [0.4520],\n",
       "        [0.4588]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction\n",
    "with torch.inference_mode(): # inference mode is a context manager that disables gradient tracking\n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f5b39cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8600],\n",
       "        [0.8740],\n",
       "        [0.8880],\n",
       "        [0.9020],\n",
       "        [0.9160],\n",
       "        [0.9300],\n",
       "        [0.9440],\n",
       "        [0.9580],\n",
       "        [0.9720],\n",
       "        [0.9860]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "aa692882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7943615",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "\n",
    "Move from some (unknown) parameters to known/expected parameters, or from a poor representation of the data to a better representation of the data\n",
    "\n",
    "One way to measure the accuracy of the model is to use a **Loss function**.\n",
    "\n",
    "**Optimizer** takes into account the loss of a model and adjusts the model's parameters (weights and biases) to reduce the loss.\n",
    "\n",
    "For PyTorch, we need:\n",
    "- A training loop\n",
    "- A testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fb9568ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9962bfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e93d15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a loss function\n",
    "Loss_fn = nn.L1Loss() # Mean Absolute Error (MAE)\n",
    "\n",
    "# Set up an optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.001) # Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2a908",
   "metadata": {},
   "source": [
    "### Building training loop and testing loop in PyTorch\n",
    "\n",
    "What we need in training loop:\n",
    "\n",
    "0. Loop through the data\n",
    "1. Forward pass/propagation (data is taken through `forward` method)\n",
    "2. Calculate the loss (compare forward pass predictions to ground truth labels)\n",
    "3. Optimizer zero grad\n",
    "4. Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss (**backpropagation**)\n",
    "5. Optimizer step - Use the optimizer to adjust our model's parameter to try and improve the loss (**gradient descent**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "89ecec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init results for observation (store actual values, not references)\n",
    "results = []\n",
    "params = list(model_0.parameters())\n",
    "results.append((params[0].item(), params[1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1492a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.31288138031959534\n",
      "Loss: 0.3117292523384094\n",
      "Loss: 0.3105771541595459\n",
      "Loss: 0.3094250559806824\n",
      "Loss: 0.30827295780181885\n",
      "Loss: 0.3071208596229553\n",
      "Loss: 0.3059687614440918\n",
      "Loss: 0.3048166334629059\n",
      "Loss: 0.30366456508636475\n",
      "Loss: 0.3025124669075012\n",
      "Loss: 0.3013603389263153\n",
      "Loss: 0.30020827054977417\n",
      "Loss: 0.29905614256858826\n",
      "Loss: 0.2979040741920471\n",
      "Loss: 0.2967519462108612\n",
      "Loss: 0.29559987783432007\n",
      "Loss: 0.29444774985313416\n",
      "Loss: 0.293295681476593\n",
      "Loss: 0.2921435832977295\n",
      "Loss: 0.2909914553165436\n",
      "Loss: 0.28983938694000244\n",
      "Loss: 0.28868725895881653\n",
      "Loss: 0.287535160779953\n",
      "Loss: 0.2863830626010895\n",
      "Loss: 0.28523099422454834\n",
      "Loss: 0.2840788662433624\n",
      "Loss: 0.2829267382621765\n",
      "Loss: 0.281774640083313\n",
      "Loss: 0.28062254190444946\n",
      "Loss: 0.2794705033302307\n",
      "Loss: 0.2783183455467224\n",
      "Loss: 0.2771662771701813\n",
      "Loss: 0.27601414918899536\n",
      "Loss: 0.2748620808124542\n",
      "Loss: 0.2737099528312683\n",
      "Loss: 0.2725578844547272\n",
      "Loss: 0.27140578627586365\n",
      "Loss: 0.2702536880970001\n",
      "Loss: 0.2691015899181366\n",
      "Loss: 0.26794949173927307\n",
      "Loss: 0.26679736375808716\n",
      "Loss: 0.265645295381546\n",
      "Loss: 0.2644931674003601\n",
      "Loss: 0.2633410692214966\n",
      "Loss: 0.26218897104263306\n",
      "Loss: 0.26103687286376953\n",
      "Loss: 0.259884774684906\n",
      "Loss: 0.2587326467037201\n",
      "Loss: 0.25758057832717896\n",
      "Loss: 0.2564285099506378\n",
      "Loss: 0.2552763819694519\n",
      "Loss: 0.2541242837905884\n",
      "Loss: 0.25297218561172485\n",
      "Loss: 0.2518201172351837\n",
      "Loss: 0.2506679892539978\n",
      "Loss: 0.24951589107513428\n",
      "Loss: 0.24836377799510956\n",
      "Loss: 0.24721169471740723\n",
      "Loss: 0.2460596114397049\n",
      "Loss: 0.2449074685573578\n",
      "Loss: 0.24375538527965546\n",
      "Loss: 0.24260330200195312\n",
      "Loss: 0.2414511889219284\n",
      "Loss: 0.2402990758419037\n",
      "Loss: 0.23914699256420135\n",
      "Loss: 0.23799486458301544\n",
      "Loss: 0.2368427962064743\n",
      "Loss: 0.23569071292877197\n",
      "Loss: 0.23453858494758606\n",
      "Loss: 0.23338651657104492\n",
      "Loss: 0.232234388589859\n",
      "Loss: 0.23108229041099548\n",
      "Loss: 0.22993019223213196\n",
      "Loss: 0.22877809405326843\n",
      "Loss: 0.2276259958744049\n",
      "Loss: 0.22647389769554138\n",
      "Loss: 0.22532181441783905\n",
      "Loss: 0.22416970133781433\n",
      "Loss: 0.223017618060112\n",
      "Loss: 0.2218654900789261\n",
      "Loss: 0.22071340680122375\n",
      "Loss: 0.21956129372119904\n",
      "Loss: 0.2184092104434967\n",
      "Loss: 0.21725709736347198\n",
      "Loss: 0.21610502898693085\n",
      "Loss: 0.21495290100574493\n",
      "Loss: 0.2138008177280426\n",
      "Loss: 0.21264871954917908\n",
      "Loss: 0.21149662137031555\n",
      "Loss: 0.21034450829029083\n",
      "Loss: 0.2091923952102661\n",
      "Loss: 0.20804031193256378\n",
      "Loss: 0.20688819885253906\n",
      "Loss: 0.20573611557483673\n",
      "Loss: 0.2045840322971344\n",
      "Loss: 0.20343191921710968\n",
      "Loss: 0.20227983593940735\n",
      "Loss: 0.20112769305706024\n",
      "Loss: 0.1999756097793579\n",
      "Loss: 0.1988234966993332\n",
      "Loss: 0.19767141342163086\n",
      "Loss: 0.19651933014392853\n",
      "Loss: 0.19536720216274261\n",
      "Loss: 0.19421511888504028\n",
      "Loss: 0.19306302070617676\n",
      "Loss: 0.19191092252731323\n",
      "Loss: 0.19075879454612732\n",
      "Loss: 0.189606711268425\n",
      "Loss: 0.18845462799072266\n",
      "Loss: 0.18730251491069794\n",
      "Loss: 0.18615040183067322\n",
      "Loss: 0.18499833345413208\n",
      "Loss: 0.18384622037410736\n",
      "Loss: 0.18269412219524384\n",
      "Loss: 0.1815420240163803\n",
      "Loss: 0.18038992583751678\n",
      "Loss: 0.17923781275749207\n",
      "Loss: 0.17808572947978973\n",
      "Loss: 0.17693361639976501\n",
      "Loss: 0.1757815182209015\n",
      "Loss: 0.17462942004203796\n",
      "Loss: 0.17347732186317444\n",
      "Loss: 0.1723252385854721\n",
      "Loss: 0.17117314040660858\n",
      "Loss: 0.17002105712890625\n",
      "Loss: 0.1688689887523651\n",
      "Loss: 0.16771690547466278\n",
      "Loss: 0.16656479239463806\n",
      "Loss: 0.16541273891925812\n",
      "Loss: 0.1642606407403946\n",
      "Loss: 0.16310855746269226\n",
      "Loss: 0.16195647418498993\n",
      "Loss: 0.1608043909072876\n",
      "Loss: 0.15965232253074646\n",
      "Loss: 0.15850022435188293\n",
      "Loss: 0.1573481261730194\n",
      "Loss: 0.15619607269763947\n",
      "Loss: 0.15504398941993713\n",
      "Loss: 0.1538918912410736\n",
      "Loss: 0.15273980796337128\n",
      "Loss: 0.15158770978450775\n",
      "Loss: 0.1504356414079666\n",
      "Loss: 0.14928355813026428\n",
      "Loss: 0.14813147485256195\n",
      "Loss: 0.14697939157485962\n",
      "Loss: 0.1458273082971573\n",
      "Loss: 0.14467521011829376\n",
      "Loss: 0.14352312684059143\n",
      "Loss: 0.1423710286617279\n",
      "Loss: 0.14121896028518677\n",
      "Loss: 0.14006686210632324\n",
      "Loss: 0.1389147937297821\n",
      "Loss: 0.13776269555091858\n",
      "Loss: 0.13661061227321625\n",
      "Loss: 0.1354585438966751\n",
      "Loss: 0.13430646061897278\n",
      "Loss: 0.13315437734127045\n",
      "Loss: 0.13200227916240692\n",
      "Loss: 0.13085021078586578\n",
      "Loss: 0.12969812750816345\n",
      "Loss: 0.12854602932929993\n",
      "Loss: 0.1273939311504364\n",
      "Loss: 0.12624187767505646\n",
      "Loss: 0.12508977949619293\n",
      "Loss: 0.12393768876791\n",
      "Loss: 0.12278560549020767\n",
      "Loss: 0.12163352966308594\n",
      "Loss: 0.1204814463853836\n",
      "Loss: 0.11932935565710068\n",
      "Loss: 0.11817727237939835\n",
      "Loss: 0.11702518165111542\n",
      "Loss: 0.11587311327457428\n",
      "Loss: 0.11476147174835205\n",
      "Loss: 0.11370686441659927\n",
      "Loss: 0.1126522570848465\n",
      "Loss: 0.11159764230251312\n",
      "Loss: 0.11054304987192154\n",
      "Loss: 0.10948844254016876\n",
      "Loss: 0.10846539586782455\n",
      "Loss: 0.10750406980514526\n",
      "Loss: 0.10654274374246597\n",
      "Loss: 0.10558142513036728\n",
      "Loss: 0.10462009906768799\n",
      "Loss: 0.1036587730050087\n",
      "Loss: 0.10270978510379791\n",
      "Loss: 0.10183751583099365\n",
      "Loss: 0.10096526145935059\n",
      "Loss: 0.10009298473596573\n",
      "Loss: 0.09922071546316147\n",
      "Loss: 0.09834843873977661\n",
      "Loss: 0.09747617691755295\n",
      "Loss: 0.09663032740354538\n",
      "Loss: 0.09584285318851471\n",
      "Loss: 0.09505538642406464\n",
      "Loss: 0.09426790475845337\n",
      "Loss: 0.09348044544458389\n",
      "Loss: 0.09269297122955322\n",
      "Loss: 0.09190551191568375\n",
      "Loss: 0.09114636480808258\n",
      "Loss: 0.09043945372104645\n",
      "Loss: 0.08973254263401031\n",
      "Loss: 0.08902563154697418\n",
      "Loss: 0.08831872791051865\n",
      "Loss: 0.08761182427406311\n",
      "Loss: 0.08690490573644638\n",
      "Loss: 0.08621595799922943\n",
      "Loss: 0.0855853408575058\n",
      "Loss: 0.08495471626520157\n",
      "Loss: 0.08432409167289734\n",
      "Loss: 0.08369346708059311\n",
      "Loss: 0.08306284993886948\n",
      "Loss: 0.08243221789598465\n",
      "Loss: 0.08180160820484161\n",
      "Loss: 0.08120343089103699\n",
      "Loss: 0.08064477890729904\n",
      "Loss: 0.0800861120223999\n",
      "Loss: 0.07952746003866196\n",
      "Loss: 0.07896880060434341\n",
      "Loss: 0.07841013371944427\n",
      "Loss: 0.07785148918628693\n",
      "Loss: 0.07729282230138779\n",
      "Loss: 0.07676678895950317\n",
      "Loss: 0.07627572864294052\n",
      "Loss: 0.07578467577695847\n",
      "Loss: 0.07529362291097641\n",
      "Loss: 0.07480257004499435\n",
      "Loss: 0.0743115097284317\n",
      "Loss: 0.07382047176361084\n",
      "Loss: 0.07332941144704819\n",
      "Loss: 0.072856605052948\n",
      "Loss: 0.07242877781391144\n",
      "Loss: 0.07200097292661667\n",
      "Loss: 0.0715731531381607\n",
      "Loss: 0.07114534080028534\n",
      "Loss: 0.07071752846240997\n",
      "Loss: 0.0702897235751152\n",
      "Loss: 0.06986190378665924\n",
      "Loss: 0.06943409144878387\n",
      "Loss: 0.06902603805065155\n",
      "Loss: 0.06865701824426651\n",
      "Loss: 0.06828799843788147\n",
      "Loss: 0.06791899353265762\n",
      "Loss: 0.06754995882511139\n",
      "Loss: 0.06718094646930695\n",
      "Loss: 0.0668119341135025\n",
      "Loss: 0.06644289940595627\n",
      "Loss: 0.06607388705015182\n",
      "Loss: 0.06570921093225479\n",
      "Loss: 0.06539449840784073\n",
      "Loss: 0.06507977098226547\n",
      "Loss: 0.06476505100727081\n",
      "Loss: 0.06445033103227615\n",
      "Loss: 0.0641356036067009\n",
      "Loss: 0.06382088363170624\n",
      "Loss: 0.06350617110729218\n",
      "Loss: 0.06319145113229752\n",
      "Loss: 0.06287673115730286\n",
      "Loss: 0.0625620037317276\n",
      "Loss: 0.062271296977996826\n",
      "Loss: 0.06200631707906723\n",
      "Loss: 0.06174134090542793\n",
      "Loss: 0.06147634983062744\n",
      "Loss: 0.061211369931697845\n",
      "Loss: 0.06094638630747795\n",
      "Loss: 0.06068141385912895\n",
      "Loss: 0.06041641905903816\n",
      "Loss: 0.060151439160108566\n",
      "Loss: 0.05988645553588867\n",
      "Loss: 0.05962147191166878\n",
      "Loss: 0.05937860533595085\n",
      "Loss: 0.05915876477956772\n",
      "Loss: 0.058938927948474884\n",
      "Loss: 0.05871908739209175\n",
      "Loss: 0.05849923565983772\n",
      "Loss: 0.05827939510345459\n",
      "Loss: 0.05805954337120056\n",
      "Loss: 0.05783969908952713\n",
      "Loss: 0.057619858533144\n",
      "Loss: 0.05740001052618027\n",
      "Loss: 0.05718017369508743\n",
      "Loss: 0.0569603331387043\n",
      "Loss: 0.05676015466451645\n",
      "Loss: 0.05658075958490372\n",
      "Loss: 0.05640135332942009\n",
      "Loss: 0.05622195079922676\n",
      "Loss: 0.05604255199432373\n",
      "Loss: 0.0558631531894207\n",
      "Loss: 0.05568375438451767\n",
      "Loss: 0.05550435930490494\n",
      "Loss: 0.05532495304942131\n",
      "Loss: 0.05514555424451828\n",
      "Loss: 0.05496615171432495\n",
      "Loss: 0.05478675290942192\n",
      "Loss: 0.05460735410451889\n",
      "Loss: 0.05443967133760452\n",
      "Loss: 0.05429593846201897\n",
      "Loss: 0.05415221303701401\n",
      "Loss: 0.05400847643613815\n",
      "Loss: 0.05386475846171379\n",
      "Loss: 0.05372103303670883\n",
      "Loss: 0.05357731133699417\n",
      "Loss: 0.05343357473611832\n",
      "Loss: 0.05328984931111336\n",
      "Loss: 0.0531461238861084\n",
      "Loss: 0.05300239473581314\n",
      "Loss: 0.052858661860227585\n",
      "Loss: 0.052714936435222626\n",
      "Loss: 0.052571214735507965\n",
      "Loss: 0.05242748185992241\n",
      "Loss: 0.05229362100362778\n",
      "Loss: 0.05218071490526199\n",
      "Loss: 0.05206780880689621\n",
      "Loss: 0.05195491388440132\n",
      "Loss: 0.05184202268719673\n",
      "Loss: 0.05172910541296005\n",
      "Loss: 0.05161619931459427\n",
      "Loss: 0.05150330066680908\n",
      "Loss: 0.0513903982937336\n",
      "Loss: 0.05127749592065811\n",
      "Loss: 0.05116458982229233\n",
      "Loss: 0.05105169489979744\n",
      "Loss: 0.05093878507614136\n",
      "Loss: 0.05082588270306587\n",
      "Loss: 0.05071298032999039\n",
      "Loss: 0.0506000742316246\n",
      "Loss: 0.050487179309129715\n",
      "Loss: 0.050378382205963135\n",
      "Loss: 0.050291359424591064\n",
      "Loss: 0.050204355269670486\n",
      "Loss: 0.050117332488298416\n",
      "Loss: 0.050030313432216644\n",
      "Loss: 0.049943309277296066\n",
      "Loss: 0.0498562827706337\n",
      "Loss: 0.049769263714551926\n",
      "Loss: 0.04968224838376045\n",
      "Loss: 0.04959522932767868\n",
      "Loss: 0.049508217722177505\n",
      "Loss: 0.04942120611667633\n",
      "Loss: 0.04933418333530426\n",
      "Loss: 0.04924716800451279\n",
      "Loss: 0.049160152673721313\n",
      "Loss: 0.04907313734292984\n",
      "Loss: 0.04898611828684807\n",
      "Loss: 0.048899102956056595\n",
      "Loss: 0.04881208389997482\n",
      "Loss: 0.04872506856918335\n",
      "Loss: 0.048638053238391876\n",
      "Loss: 0.04855869710445404\n",
      "Loss: 0.048492539674043655\n",
      "Loss: 0.04842637851834297\n",
      "Loss: 0.04836020991206169\n",
      "Loss: 0.04829404503107071\n",
      "Loss: 0.04822787642478943\n",
      "Loss: 0.048161718994379044\n",
      "Loss: 0.048095546662807465\n",
      "Loss: 0.04802938550710678\n",
      "Loss: 0.0479632243514061\n",
      "Loss: 0.04789705574512482\n",
      "Loss: 0.04783089831471443\n",
      "Loss: 0.04776472598314285\n",
      "Loss: 0.04769856110215187\n",
      "Loss: 0.04763239622116089\n",
      "Loss: 0.04756623134016991\n",
      "Loss: 0.04750007390975952\n",
      "Loss: 0.04743390530347824\n",
      "Loss: 0.04736773669719696\n",
      "Loss: 0.04730157181620598\n",
      "Loss: 0.0472353920340538\n",
      "Loss: 0.047169238328933716\n",
      "Loss: 0.047103073447942734\n",
      "Loss: 0.04703690856695175\n",
      "Loss: 0.04697074741125107\n",
      "Loss: 0.04690460115671158\n",
      "Loss: 0.04685414209961891\n",
      "Loss: 0.046803683042526245\n",
      "Loss: 0.046753231436014175\n",
      "Loss: 0.04670276492834091\n",
      "Loss: 0.046652309596538544\n",
      "Loss: 0.04660185053944588\n",
      "Loss: 0.04655139893293381\n",
      "Loss: 0.04650093987584114\n",
      "Loss: 0.04645047336816788\n",
      "Loss: 0.04640001803636551\n",
      "Loss: 0.046349555253982544\n",
      "Loss: 0.04629909247159958\n",
      "Loss: 0.04624864086508751\n",
      "Loss: 0.046198178082704544\n",
      "Loss: 0.046147722750902176\n",
      "Loss: 0.04609726741909981\n",
      "Loss: 0.04604680463671684\n",
      "Loss: 0.045996345579624176\n",
      "Loss: 0.04594588279724121\n",
      "Loss: 0.04589542746543884\n",
      "Loss: 0.04584496468305588\n",
      "Loss: 0.045794516801834106\n",
      "Loss: 0.04574405029416084\n",
      "Loss: 0.045693591237068176\n",
      "Loss: 0.04564313963055611\n",
      "Loss: 0.04559267684817314\n",
      "Loss: 0.045542217791080475\n",
      "Loss: 0.04549176245927811\n",
      "Loss: 0.04544129967689514\n",
      "Loss: 0.04539085179567337\n",
      "Loss: 0.04534038156270981\n",
      "Loss: 0.04528992623090744\n",
      "Loss: 0.04523947089910507\n",
      "Loss: 0.045189011842012405\n",
      "Loss: 0.04513854905962944\n",
      "Loss: 0.04509454965591431\n",
      "Loss: 0.045054562389850616\n",
      "Loss: 0.045014552772045135\n",
      "Loss: 0.04497454687952995\n",
      "Loss: 0.04493454843759537\n",
      "Loss: 0.04489455372095108\n",
      "Loss: 0.044854551553726196\n",
      "Loss: 0.044814541935920715\n",
      "Loss: 0.04477455094456673\n",
      "Loss: 0.044734545052051544\n",
      "Loss: 0.04469455033540726\n",
      "Loss: 0.04465455189347267\n",
      "Loss: 0.04461454600095749\n",
      "Loss: 0.0445745475590229\n",
      "Loss: 0.04453454539179802\n",
      "Loss: 0.044494546949863434\n",
      "Loss: 0.04445454478263855\n",
      "Loss: 0.044414542615413666\n",
      "Loss: 0.04437453672289848\n",
      "Loss: 0.0443345345556736\n",
      "Loss: 0.04429454356431961\n",
      "Loss: 0.04425454139709473\n",
      "Loss: 0.04421453922986984\n",
      "Loss: 0.04417454078793526\n",
      "Loss: 0.04413453862071037\n",
      "Loss: 0.04409453272819519\n",
      "Loss: 0.0440545380115509\n",
      "Loss: 0.04401453956961632\n",
      "Loss: 0.04397452995181084\n",
      "Loss: 0.04393453150987625\n",
      "Loss: 0.043894533067941666\n",
      "Loss: 0.04385453090071678\n",
      "Loss: 0.043814532458782196\n",
      "Loss: 0.043774526566267014\n",
      "Loss: 0.043734531849622726\n",
      "Loss: 0.04369453340768814\n",
      "Loss: 0.04365452378988266\n",
      "Loss: 0.04361452907323837\n",
      "Loss: 0.04357453063130379\n",
      "Loss: 0.043534524738788605\n",
      "Loss: 0.04349452629685402\n",
      "Loss: 0.043454527854919434\n",
      "Loss: 0.04341452196240425\n",
      "Loss: 0.043374527245759964\n",
      "Loss: 0.04333452507853508\n",
      "Loss: 0.043294526636600494\n",
      "Loss: 0.04325452446937561\n",
      "Loss: 0.04321451857686043\n",
      "Loss: 0.04317452758550644\n",
      "Loss: 0.04313452169299126\n",
      "Loss: 0.04309451952576637\n",
      "Loss: 0.04305451363325119\n",
      "Loss: 0.043014515191316605\n",
      "Loss: 0.04297452047467232\n",
      "Loss: 0.042934514582157135\n",
      "Loss: 0.04289551451802254\n",
      "Loss: 0.04286060854792595\n",
      "Loss: 0.04282570630311966\n",
      "Loss: 0.04279080033302307\n",
      "Loss: 0.04275590553879738\n",
      "Loss: 0.042720992118120193\n",
      "Loss: 0.0426860973238945\n",
      "Loss: 0.04265119880437851\n",
      "Loss: 0.04261629283428192\n",
      "Loss: 0.04258139431476593\n",
      "Loss: 0.04254649952054024\n",
      "Loss: 0.04251159727573395\n",
      "Loss: 0.04247669130563736\n",
      "Loss: 0.04244178533554077\n",
      "Loss: 0.04240688681602478\n",
      "Loss: 0.042371977120637894\n",
      "Loss: 0.0423370823264122\n",
      "Loss: 0.04230218380689621\n",
      "Loss: 0.04226727783679962\n",
      "Loss: 0.04223238304257393\n",
      "Loss: 0.04219748452305794\n",
      "Loss: 0.04216257855296135\n",
      "Loss: 0.04212767630815506\n",
      "Loss: 0.04209277033805847\n",
      "Loss: 0.04205787554383278\n",
      "Loss: 0.042022962123155594\n",
      "Loss: 0.0419880673289299\n",
      "Loss: 0.04195316880941391\n",
      "Loss: 0.04191826283931732\n",
      "Loss: 0.04188336431980133\n",
      "Loss: 0.04184846952557564\n",
      "Loss: 0.04181356728076935\n",
      "Loss: 0.04177866131067276\n",
      "Loss: 0.04174375534057617\n",
      "Loss: 0.04170885682106018\n",
      "Loss: 0.041673947125673294\n",
      "Loss: 0.0416390523314476\n",
      "Loss: 0.04160415381193161\n",
      "Loss: 0.04156924784183502\n",
      "Loss: 0.04153435304760933\n",
      "Loss: 0.04149945452809334\n",
      "Loss: 0.04146454855799675\n",
      "Loss: 0.04142964631319046\n",
      "Loss: 0.04139474034309387\n",
      "Loss: 0.04135984554886818\n",
      "Loss: 0.041324932128190994\n",
      "Loss: 0.0412900373339653\n",
      "Loss: 0.04125513881444931\n",
      "Loss: 0.04122023284435272\n",
      "Loss: 0.04118533432483673\n",
      "Loss: 0.04115043953061104\n",
      "Loss: 0.04111553728580475\n",
      "Loss: 0.04108063131570816\n",
      "Loss: 0.04104572534561157\n",
      "Loss: 0.04101082682609558\n",
      "Loss: 0.040975917130708694\n",
      "Loss: 0.040941022336483\n",
      "Loss: 0.04090612381696701\n",
      "Loss: 0.04087121784687042\n",
      "Loss: 0.04083632305264473\n",
      "Loss: 0.04080142453312874\n",
      "Loss: 0.04076651856303215\n",
      "Loss: 0.04073161631822586\n",
      "Loss: 0.04069671034812927\n",
      "Loss: 0.04066181555390358\n",
      "Loss: 0.040626902133226395\n",
      "Loss: 0.0405920073390007\n",
      "Loss: 0.04055710881948471\n",
      "Loss: 0.04052220284938812\n",
      "Loss: 0.04048730432987213\n",
      "Loss: 0.04045240953564644\n",
      "Loss: 0.04041750729084015\n",
      "Loss: 0.04038260132074356\n",
      "Loss: 0.04034769535064697\n",
      "Loss: 0.04031279683113098\n",
      "Loss: 0.040277887135744095\n",
      "Loss: 0.0402429923415184\n",
      "Loss: 0.04020809382200241\n",
      "Loss: 0.04017318785190582\n",
      "Loss: 0.04013829305768013\n",
      "Loss: 0.04010339453816414\n",
      "Loss: 0.04006849601864815\n",
      "Loss: 0.04003359004855156\n",
      "Loss: 0.03999868780374527\n",
      "Loss: 0.03996378555893898\n",
      "Loss: 0.03992888331413269\n",
      "Loss: 0.0398939847946167\n",
      "Loss: 0.03985908254981041\n",
      "Loss: 0.03982418030500412\n",
      "Loss: 0.03978928178548813\n",
      "Loss: 0.039754386991262436\n",
      "Loss: 0.03971948102116585\n",
      "Loss: 0.03968457505106926\n",
      "Loss: 0.03964967280626297\n",
      "Loss: 0.03961477428674698\n",
      "Loss: 0.03957986459136009\n",
      "Loss: 0.0395449697971344\n",
      "Loss: 0.03951007127761841\n",
      "Loss: 0.03947516530752182\n",
      "Loss: 0.03944026678800583\n",
      "Loss: 0.03940536826848984\n",
      "Loss: 0.03937046602368355\n",
      "Loss: 0.03933556005358696\n",
      "Loss: 0.03930065780878067\n",
      "Loss: 0.03926575928926468\n",
      "Loss: 0.03923085331916809\n",
      "Loss: 0.0391959547996521\n",
      "Loss: 0.03916105255484581\n",
      "Loss: 0.03912615031003952\n",
      "Loss: 0.03909125179052353\n",
      "Loss: 0.039056356996297836\n",
      "Loss: 0.03902145102620125\n",
      "Loss: 0.03898654505610466\n",
      "Loss: 0.03895164281129837\n",
      "Loss: 0.03891674429178238\n",
      "Loss: 0.03888183459639549\n",
      "Loss: 0.0388469398021698\n",
      "Loss: 0.03881204128265381\n",
      "Loss: 0.03877713531255722\n",
      "Loss: 0.03874223679304123\n",
      "Loss: 0.03870733827352524\n",
      "Loss: 0.03867243602871895\n",
      "Loss: 0.03863753005862236\n",
      "Loss: 0.03860262781381607\n",
      "Loss: 0.03856772929430008\n",
      "Loss: 0.03853282332420349\n",
      "Loss: 0.0384979248046875\n",
      "Loss: 0.03846302255988121\n",
      "Loss: 0.03842812031507492\n",
      "Loss: 0.03839322179555893\n",
      "Loss: 0.03835832700133324\n",
      "Loss: 0.03832342103123665\n",
      "Loss: 0.03828851506114006\n",
      "Loss: 0.03825361281633377\n",
      "Loss: 0.03821871429681778\n",
      "Loss: 0.03818380460143089\n",
      "Loss: 0.0381489098072052\n",
      "Loss: 0.03811401128768921\n",
      "Loss: 0.03807910531759262\n",
      "Loss: 0.03804420679807663\n",
      "Loss: 0.03800930827856064\n",
      "Loss: 0.03797440603375435\n",
      "Loss: 0.03793950006365776\n",
      "Loss: 0.03790459781885147\n",
      "Loss: 0.03786969929933548\n",
      "Loss: 0.03783479332923889\n",
      "Loss: 0.0377998948097229\n",
      "Loss: 0.03776499256491661\n",
      "Loss: 0.03773009032011032\n",
      "Loss: 0.03769519180059433\n",
      "Loss: 0.03766029700636864\n",
      "Loss: 0.03762539103627205\n",
      "Loss: 0.03759048506617546\n",
      "Loss: 0.03755558282136917\n",
      "Loss: 0.03752068430185318\n",
      "Loss: 0.03748577460646629\n",
      "Loss: 0.0374508798122406\n",
      "Loss: 0.03741598129272461\n",
      "Loss: 0.03738107532262802\n",
      "Loss: 0.03734617680311203\n",
      "Loss: 0.03731127828359604\n",
      "Loss: 0.03727637603878975\n",
      "Loss: 0.03724147006869316\n",
      "Loss: 0.03720656782388687\n",
      "Loss: 0.03717166930437088\n",
      "Loss: 0.03713676333427429\n",
      "Loss: 0.0371018648147583\n",
      "Loss: 0.037067197263240814\n",
      "Loss: 0.037033338099718094\n",
      "Loss: 0.036998435854911804\n",
      "Loss: 0.036964669823646545\n",
      "Loss: 0.036929916590452194\n",
      "Loss: 0.036895763128995895\n",
      "Loss: 0.03686138987541199\n",
      "Loss: 0.036826856434345245\n",
      "Loss: 0.03679286316037178\n",
      "Loss: 0.03675796836614609\n",
      "Loss: 0.03672432154417038\n",
      "Loss: 0.03668942302465439\n",
      "Loss: 0.03665542230010033\n",
      "Loss: 0.03662090748548508\n",
      "Loss: 0.03658652305603027\n",
      "Loss: 0.03655238077044487\n",
      "Loss: 0.03651762008666992\n",
      "Loss: 0.03648385405540466\n",
      "Loss: 0.03644895553588867\n",
      "Loss: 0.03641509264707565\n",
      "Loss: 0.03638043254613876\n",
      "Loss: 0.036346182227134705\n",
      "Loss: 0.036311905831098557\n",
      "Loss: 0.03627727925777435\n",
      "Loss: 0.03624338284134865\n",
      "Loss: 0.03620847314596176\n",
      "Loss: 0.036174751818180084\n",
      "Loss: 0.03613995760679245\n",
      "Loss: 0.036105841398239136\n",
      "Loss: 0.03607143089175224\n",
      "Loss: 0.036036938428878784\n",
      "Loss: 0.03600289672613144\n",
      "Loss: 0.035968031734228134\n",
      "Loss: 0.03593437746167183\n",
      "Loss: 0.03589947521686554\n",
      "Loss: 0.035865504294633865\n",
      "Loss: 0.03583095222711563\n",
      "Loss: 0.03579659387469292\n",
      "Loss: 0.03576242923736572\n",
      "Loss: 0.03572769835591316\n",
      "Loss: 0.035693906247615814\n",
      "Loss: 0.03565899655222893\n",
      "Loss: 0.035625167191028595\n",
      "Loss: 0.03559047356247902\n",
      "Loss: 0.03555626422166824\n",
      "Loss: 0.03552195802330971\n",
      "Loss: 0.03548736125230789\n",
      "Loss: 0.0354534275829792\n",
      "Loss: 0.035418521612882614\n",
      "Loss: 0.035384830087423325\n",
      "Loss: 0.035350002348423004\n",
      "Loss: 0.035315923392772675\n",
      "Loss: 0.0352814719080925\n",
      "Loss: 0.03524700924754143\n",
      "Loss: 0.03521294146776199\n",
      "Loss: 0.03517811372876167\n",
      "Loss: 0.03514442220330238\n",
      "Loss: 0.0351095125079155\n",
      "Loss: 0.035075593739748\n",
      "Loss: 0.03504099324345589\n",
      "Loss: 0.035006679594516754\n",
      "Loss: 0.03497246652841568\n",
      "Loss: 0.0349377766251564\n",
      "Loss: 0.03490393981337547\n",
      "Loss: 0.03486904129385948\n",
      "Loss: 0.03483524173498154\n",
      "Loss: 0.03480052202939987\n",
      "Loss: 0.034766342490911484\n",
      "Loss: 0.034731991589069366\n",
      "Loss: 0.034697435796260834\n",
      "Loss: 0.03466346859931946\n",
      "Loss: 0.03462856262922287\n",
      "Loss: 0.03459491580724716\n",
      "Loss: 0.03456003591418266\n",
      "Loss: 0.034526001662015915\n",
      "Loss: 0.034491509199142456\n",
      "Loss: 0.03445709869265556\n",
      "Loss: 0.03442298620939255\n",
      "Loss: 0.03438819572329521\n",
      "Loss: 0.03435446694493294\n",
      "Loss: 0.03431956097483635\n",
      "Loss: 0.034285664558410645\n",
      "Loss: 0.034251030534505844\n",
      "Loss: 0.03421676158905029\n",
      "Loss: 0.034182511270046234\n",
      "Loss: 0.034147851169109344\n",
      "Loss: 0.03411398082971573\n",
      "Loss: 0.03407908231019974\n",
      "Loss: 0.03404533118009567\n",
      "Loss: 0.03401055932044983\n",
      "Loss: 0.03397642448544502\n",
      "Loss: 0.03394203260540962\n",
      "Loss: 0.033907510340213776\n",
      "Loss: 0.03387351706624031\n",
      "Loss: 0.033838607370853424\n",
      "Loss: 0.03380498290061951\n",
      "Loss: 0.033770088106393814\n",
      "Loss: 0.033736079931259155\n",
      "Loss: 0.03370155766606331\n",
      "Loss: 0.033667173236608505\n",
      "Loss: 0.0336330309510231\n",
      "Loss: 0.033598270267248154\n",
      "Loss: 0.033564500510692596\n",
      "Loss: 0.033529605716466904\n",
      "Loss: 0.03349574655294418\n",
      "Loss: 0.0334610790014267\n",
      "Loss: 0.03342684358358383\n",
      "Loss: 0.03339255228638649\n",
      "Loss: 0.03335793316364288\n",
      "Loss: 0.03332402929663658\n",
      "Loss: 0.03328912332653999\n",
      "Loss: 0.033255405724048615\n",
      "Loss: 0.033220600336790085\n",
      "Loss: 0.03318650275468826\n",
      "Loss: 0.03315207362174988\n",
      "Loss: 0.03311759606003761\n",
      "Loss: 0.03308354690670967\n",
      "Loss: 0.033048685640096664\n",
      "Loss: 0.033015020191669464\n",
      "Loss: 0.03298012539744377\n",
      "Loss: 0.032946161925792694\n",
      "Loss: 0.032911598682403564\n",
      "Loss: 0.03287725895643234\n",
      "Loss: 0.03284307196736336\n",
      "Loss: 0.03280835971236229\n",
      "Loss: 0.03277454525232315\n",
      "Loss: 0.032739631831645966\n",
      "Loss: 0.032705824822187424\n",
      "Loss: 0.032671116292476654\n",
      "Loss: 0.03263692185282707\n",
      "Loss: 0.03260258957743645\n",
      "Loss: 0.03256801888346672\n",
      "Loss: 0.03253406286239624\n",
      "Loss: 0.03249916434288025\n",
      "Loss: 0.03246549516916275\n",
      "Loss: 0.03243063762784004\n",
      "Loss: 0.0323965810239315\n",
      "Loss: 0.032362114638090134\n",
      "Loss: 0.03232767805457115\n",
      "Loss: 0.032293591648340225\n",
      "Loss: 0.0322587676346302\n",
      "Loss: 0.03222506120800972\n",
      "Loss: 0.03219016641378403\n",
      "Loss: 0.03215624764561653\n",
      "Loss: 0.03212163969874382\n",
      "Loss: 0.03208733722567558\n",
      "Loss: 0.032053105533123016\n",
      "Loss: 0.03201843053102493\n",
      "Loss: 0.03198458254337311\n",
      "Loss: 0.031949687749147415\n",
      "Loss: 0.031915903091430664\n",
      "Loss: 0.03188116103410721\n",
      "Loss: 0.031846996396780014\n",
      "Loss: 0.0318126380443573\n",
      "Loss: 0.03177809715270996\n",
      "Loss: 0.03174411505460739\n",
      "Loss: 0.031709205359220505\n",
      "Loss: 0.03167556971311569\n",
      "Loss: 0.031640682369470596\n",
      "Loss: 0.03160666301846504\n",
      "Loss: 0.031572166830301285\n",
      "Loss: 0.03153776004910469\n",
      "Loss: 0.03150363638997078\n",
      "Loss: 0.03146884962916374\n",
      "Loss: 0.03143510967493057\n",
      "Loss: 0.03140020743012428\n",
      "Loss: 0.03136632218956947\n",
      "Loss: 0.031331680715084076\n",
      "Loss: 0.031297408044338226\n",
      "Loss: 0.03126315027475357\n",
      "Loss: 0.03122851625084877\n",
      "Loss: 0.03119463101029396\n",
      "Loss: 0.031159725040197372\n",
      "Loss: 0.0311259925365448\n",
      "Loss: 0.031091203913092613\n",
      "Loss: 0.031057080253958702\n",
      "Loss: 0.031022677198052406\n",
      "Loss: 0.03098817728459835\n",
      "Loss: 0.03095415234565735\n",
      "Loss: 0.03091926872730255\n",
      "Loss: 0.03088562563061714\n",
      "Loss: 0.03085072711110115\n",
      "Loss: 0.03081674501299858\n",
      "Loss: 0.030782198533415794\n",
      "Loss: 0.03074783645570278\n",
      "Loss: 0.030713677406311035\n",
      "Loss: 0.030678927898406982\n",
      "Loss: 0.030645150691270828\n",
      "Loss: 0.03061024472117424\n",
      "Loss: 0.030576402321457863\n",
      "Loss: 0.03054172359406948\n",
      "Loss: 0.03050749935209751\n",
      "Loss: 0.030473193153738976\n",
      "Loss: 0.03043859638273716\n",
      "Loss: 0.030404681339859962\n",
      "Loss: 0.030369769781827927\n",
      "Loss: 0.030336061492562294\n",
      "Loss: 0.03030124306678772\n",
      "Loss: 0.030267158523201942\n",
      "Loss: 0.03023272193968296\n",
      "Loss: 0.030198251828551292\n",
      "Loss: 0.030164187774062157\n",
      "Loss: 0.030129343271255493\n",
      "Loss: 0.030095672234892845\n",
      "Loss: 0.030060768127441406\n",
      "Loss: 0.03002682328224182\n",
      "Loss: 0.0299922414124012\n",
      "Loss: 0.02995791658759117\n",
      "Loss: 0.02992371842265129\n",
      "Loss: 0.029889006167650223\n",
      "Loss: 0.029855191707611084\n",
      "Loss: 0.029820293188095093\n",
      "Loss: 0.029786478728055954\n",
      "Loss: 0.02975175902247429\n",
      "Loss: 0.029717573896050453\n",
      "Loss: 0.02968323789536953\n",
      "Loss: 0.0296486709266901\n",
      "Loss: 0.029614707455039024\n",
      "Loss: 0.02957981266081333\n",
      "Loss: 0.02954614721238613\n",
      "Loss: 0.029511287808418274\n",
      "Loss: 0.029477238655090332\n",
      "Loss: 0.029442761093378067\n",
      "Loss: 0.029408331960439682\n",
      "Loss: 0.02937423624098301\n",
      "Loss: 0.02933942899107933\n",
      "Loss: 0.02930571138858795\n",
      "Loss: 0.029270809143781662\n",
      "Loss: 0.02923690341413021\n",
      "Loss: 0.029202282428741455\n",
      "Loss: 0.02916799485683441\n",
      "Loss: 0.029133755713701248\n",
      "Loss: 0.029099086299538612\n",
      "Loss: 0.029065227136015892\n",
      "Loss: 0.02903033420443535\n",
      "Loss: 0.028996562585234642\n",
      "Loss: 0.02896180748939514\n",
      "Loss: 0.02892765775322914\n",
      "Loss: 0.028893280774354935\n",
      "Loss: 0.02885875664651394\n",
      "Loss: 0.028824755921959877\n",
      "Loss: 0.028789842501282692\n",
      "Loss: 0.02875622548162937\n",
      "Loss: 0.02872132696211338\n",
      "Loss: 0.02868732251226902\n",
      "Loss: 0.028652798384428024\n",
      "Loss: 0.02861841954290867\n",
      "Loss: 0.028584271669387817\n",
      "Loss: 0.02854951098561287\n",
      "Loss: 0.028515750542283058\n",
      "Loss: 0.02848084643483162\n",
      "Loss: 0.02844698168337345\n",
      "Loss: 0.02841232344508171\n",
      "Loss: 0.0283780749887228\n",
      "Loss: 0.028343800455331802\n",
      "Loss: 0.028309166431427002\n",
      "Loss: 0.028275271877646446\n",
      "Loss: 0.028240377083420753\n",
      "Loss: 0.02820664644241333\n",
      "Loss: 0.028171848505735397\n",
      "Loss: 0.02813773788511753\n",
      "Loss: 0.028103316202759743\n",
      "Loss: 0.02806883119046688\n",
      "Loss: 0.028034795075654984\n",
      "Loss: 0.02799992822110653\n",
      "Loss: 0.027966270223259926\n",
      "Loss: 0.027931367978453636\n",
      "Loss: 0.027897397056221962\n",
      "Loss: 0.027862846851348877\n",
      "Loss: 0.02782849594950676\n",
      "Loss: 0.027794325724244118\n",
      "Loss: 0.02775958739221096\n",
      "Loss: 0.027725791558623314\n",
      "Loss: 0.027690891176462173\n",
      "Loss: 0.02765706181526184\n",
      "Loss: 0.027622371912002563\n",
      "Loss: 0.02758815884590149\n",
      "Loss: 0.027553845196962357\n",
      "Loss: 0.02751925028860569\n",
      "Loss: 0.027485316619277\n",
      "Loss: 0.02745041623711586\n",
      "Loss: 0.02741672471165657\n",
      "Loss: 0.027381891384720802\n",
      "Loss: 0.027347808703780174\n",
      "Loss: 0.027313362807035446\n",
      "Loss: 0.02727891504764557\n",
      "Loss: 0.02724483609199524\n",
      "Loss: 0.02721000649034977\n",
      "Loss: 0.027176309376955032\n",
      "Loss: 0.02714141272008419\n",
      "Loss: 0.0271074827760458\n",
      "Loss: 0.027072886005043983\n",
      "Loss: 0.0270385779440403\n",
      "Loss: 0.027004361152648926\n",
      "Loss: 0.0269696656614542\n",
      "Loss: 0.02693583443760872\n",
      "Loss: 0.026900935918092728\n",
      "Loss: 0.02686714567244053\n",
      "Loss: 0.02683240734040737\n",
      "Loss: 0.02679823711514473\n",
      "Loss: 0.026763886213302612\n",
      "Loss: 0.02672932669520378\n",
      "Loss: 0.026695359498262405\n",
      "Loss: 0.026660453528165817\n",
      "Loss: 0.02662680111825466\n",
      "Loss: 0.026591932401061058\n",
      "Loss: 0.02655789814889431\n",
      "Loss: 0.026523401960730553\n",
      "Loss: 0.026488998904824257\n",
      "Loss: 0.02645489014685154\n",
      "Loss: 0.02642008289694786\n",
      "Loss: 0.026386354118585587\n",
      "Loss: 0.026351448148489\n",
      "Loss: 0.02631755731999874\n",
      "Loss: 0.026282930746674538\n",
      "Loss: 0.02624865248799324\n",
      "Loss: 0.026214396581053734\n",
      "Loss: 0.026179740205407143\n",
      "Loss: 0.026145881041884422\n",
      "Loss: 0.026110976934432983\n",
      "Loss: 0.02607722207903862\n",
      "Loss: 0.026042452082037926\n",
      "Loss: 0.02600831352174282\n",
      "Loss: 0.025973927229642868\n",
      "Loss: 0.02593940868973732\n",
      "Loss: 0.02590540051460266\n",
      "Loss: 0.025870507583022118\n",
      "Loss: 0.025836873799562454\n",
      "Loss: 0.025801967829465866\n",
      "Loss: 0.025767972692847252\n",
      "Loss: 0.025733450427651405\n",
      "Loss: 0.0256990734487772\n",
      "Loss: 0.0256649199873209\n",
      "Loss: 0.02563016675412655\n",
      "Loss: 0.025596395134925842\n",
      "Loss: 0.02556149661540985\n",
      "Loss: 0.02552764117717743\n",
      "Loss: 0.025492969900369644\n",
      "Loss: 0.02545873448252678\n",
      "Loss: 0.025424445047974586\n",
      "Loss: 0.02538982965052128\n",
      "Loss: 0.02535592019557953\n",
      "Loss: 0.02532101795077324\n",
      "Loss: 0.02528730034828186\n",
      "Loss: 0.025252491235733032\n",
      "Loss: 0.02521839737892151\n",
      "Loss: 0.025183964520692825\n",
      "Loss: 0.02514948509633541\n",
      "Loss: 0.02511543594300747\n",
      "Loss: 0.025080587714910507\n",
      "Loss: 0.02504691109061241\n",
      "Loss: 0.02501201629638672\n",
      "Loss: 0.02497805655002594\n",
      "Loss: 0.024943487718701363\n",
      "Loss: 0.024909157305955887\n",
      "Loss: 0.024874964728951454\n",
      "Loss: 0.024840235710144043\n",
      "Loss: 0.024806439876556396\n",
      "Loss: 0.024771535769104958\n",
      "Loss: 0.02473771944642067\n",
      "Loss: 0.0247030109167099\n",
      "Loss: 0.024668816477060318\n",
      "Loss: 0.024634480476379395\n",
      "Loss: 0.024599911645054817\n",
      "Loss: 0.024565961211919785\n",
      "Loss: 0.024531055241823196\n",
      "Loss: 0.02449738048017025\n",
      "Loss: 0.024462532252073288\n",
      "Loss: 0.02442847564816475\n",
      "Loss: 0.02439401112496853\n",
      "Loss: 0.02435956709086895\n",
      "Loss: 0.024325478821992874\n",
      "Loss: 0.024290667846798897\n",
      "Loss: 0.024256955832242966\n",
      "Loss: 0.024222057312726974\n",
      "Loss: 0.02418813668191433\n",
      "Loss: 0.02415352500975132\n",
      "Loss: 0.02411923184990883\n",
      "Loss: 0.02408500388264656\n",
      "Loss: 0.024050328880548477\n",
      "Loss: 0.024016480892896652\n",
      "Loss: 0.023981576785445213\n",
      "Loss: 0.02394779585301876\n",
      "Loss: 0.023913055658340454\n",
      "Loss: 0.023878900334239006\n",
      "Loss: 0.023844534531235695\n",
      "Loss: 0.02380998805165291\n",
      "Loss: 0.02377600036561489\n",
      "Loss: 0.0237411018460989\n",
      "Loss: 0.02370746247470379\n",
      "Loss: 0.02367258258163929\n",
      "Loss: 0.023638557642698288\n",
      "Loss: 0.023604054003953934\n",
      "Loss: 0.023569650948047638\n",
      "Loss: 0.02353552170097828\n",
      "Loss: 0.023500746116042137\n",
      "Loss: 0.02346699871122837\n",
      "Loss: 0.02343210019171238\n",
      "Loss: 0.023398209363222122\n",
      "Loss: 0.023363569751381874\n",
      "Loss: 0.023329313844442368\n",
      "Loss: 0.023295046761631966\n",
      "Loss: 0.02326040528714657\n",
      "Loss: 0.02322651818394661\n",
      "Loss: 0.023191621527075768\n",
      "Loss: 0.023157883435487747\n",
      "Loss: 0.02312309481203556\n",
      "Loss: 0.023088976740837097\n",
      "Loss: 0.023054569959640503\n",
      "Loss: 0.02302006632089615\n",
      "Loss: 0.022986043244600296\n",
      "Loss: 0.022951165214180946\n",
      "Loss: 0.022917520254850388\n",
      "Loss: 0.02288261614739895\n",
      "Loss: 0.02284863591194153\n",
      "Loss: 0.02281409129500389\n",
      "Loss: 0.022779732942581177\n",
      "Loss: 0.022745568305253983\n",
      "Loss: 0.022710826247930527\n",
      "Loss: 0.02267703041434288\n",
      "Loss: 0.022642139345407486\n",
      "Loss: 0.022608298808336258\n",
      "Loss: 0.02257360890507698\n",
      "Loss: 0.022539397701621056\n",
      "Loss: 0.022505097091197968\n",
      "Loss: 0.02247048355638981\n",
      "Loss: 0.022436564788222313\n",
      "Loss: 0.022401660680770874\n",
      "Loss: 0.02236795797944069\n",
      "Loss: 0.022333137691020966\n",
      "Loss: 0.02229905314743519\n",
      "Loss: 0.02226460725069046\n",
      "Loss: 0.02223013900220394\n",
      "Loss: 0.02219608798623085\n",
      "Loss: 0.022161245346069336\n",
      "Loss: 0.022127559408545494\n",
      "Loss: 0.022092660889029503\n",
      "Loss: 0.02205871418118477\n",
      "Loss: 0.022024136036634445\n",
      "Loss: 0.021989809349179268\n",
      "Loss: 0.02195560745894909\n",
      "Loss: 0.021920906379818916\n",
      "Loss: 0.02188708260655403\n",
      "Loss: 0.02185218036174774\n",
      "Loss: 0.0218183733522892\n",
      "Loss: 0.021783655509352684\n",
      "Loss: 0.02174947038292885\n",
      "Loss: 0.021715128794312477\n",
      "Loss: 0.021680567413568497\n",
      "Loss: 0.02164660580456257\n",
      "Loss: 0.021611705422401428\n",
      "Loss: 0.021578039973974228\n",
      "Loss: 0.02154317870736122\n",
      "Loss: 0.021509135141968727\n",
      "Loss: 0.021474653854966164\n",
      "Loss: 0.021440228447318077\n",
      "Loss: 0.021406125277280807\n",
      "Loss: 0.021371323615312576\n",
      "Loss: 0.02133760042488575\n",
      "Loss: 0.02130269818007946\n",
      "Loss: 0.021268798038363457\n",
      "Loss: 0.021234173327684402\n",
      "Loss: 0.021199887618422508\n",
      "Loss: 0.021165648475289345\n",
      "Loss: 0.021130988374352455\n",
      "Loss: 0.02109711989760399\n",
      "Loss: 0.021062226966023445\n",
      "Loss: 0.02102845534682274\n",
      "Loss: 0.02099369652569294\n",
      "Loss: 0.020959556102752686\n",
      "Loss: 0.02092517353594303\n",
      "Loss: 0.02089063450694084\n",
      "Loss: 0.020856648683547974\n",
      "Loss: 0.020821744576096535\n",
      "Loss: 0.020788121968507767\n",
      "Loss: 0.020753219723701477\n",
      "Loss: 0.020719215273857117\n",
      "Loss: 0.02068468928337097\n",
      "Loss: 0.020650310441851616\n",
      "Loss: 0.02061617001891136\n",
      "Loss: 0.020581401884555817\n",
      "Loss: 0.020547647029161453\n",
      "Loss: 0.020512741059064865\n",
      "Loss: 0.020478874444961548\n",
      "Loss: 0.020444219931960106\n",
      "Loss: 0.020409967750310898\n",
      "Loss: 0.02037569135427475\n",
      "Loss: 0.020341066643595695\n",
      "Loss: 0.020307164639234543\n",
      "Loss: 0.02027226611971855\n",
      "Loss: 0.020238537341356277\n",
      "Loss: 0.020203733816742897\n",
      "Loss: 0.020169630646705627\n",
      "Loss: 0.020135212689638138\n",
      "Loss: 0.020100727677345276\n",
      "Loss: 0.02006668969988823\n",
      "Loss: 0.020031820982694626\n",
      "Loss: 0.01999816671013832\n",
      "Loss: 0.01996326446533203\n",
      "Loss: 0.019929300993680954\n",
      "Loss: 0.019894743338227272\n",
      "Loss: 0.019860386848449707\n",
      "Loss: 0.019826212897896767\n",
      "Loss: 0.019791483879089355\n",
      "Loss: 0.01975768432021141\n",
      "Loss: 0.019722791388630867\n",
      "Loss: 0.019688956439495087\n",
      "Loss: 0.01965426094830036\n",
      "Loss: 0.019620049744844437\n",
      "Loss: 0.019585730507969856\n",
      "Loss: 0.019551146775484085\n",
      "Loss: 0.019517207518219948\n",
      "Loss: 0.019482305273413658\n",
      "Loss: 0.01944860816001892\n",
      "Loss: 0.01941377855837345\n",
      "Loss: 0.019379712641239166\n",
      "Loss: 0.019345253705978394\n",
      "Loss: 0.019310805946588516\n",
      "Loss: 0.019276728853583336\n",
      "Loss: 0.019241899251937866\n",
      "Loss: 0.019208211451768875\n",
      "Loss: 0.019173303619027138\n",
      "Loss: 0.019139375537633896\n",
      "Loss: 0.01910477876663208\n",
      "Loss: 0.019070465117692947\n",
      "Loss: 0.019036252051591873\n",
      "Loss: 0.019001564010977745\n",
      "Loss: 0.018967729061841965\n",
      "Loss: 0.018932823091745377\n",
      "Loss: 0.018899032846093178\n",
      "Loss: 0.018864300101995468\n",
      "Loss: 0.018830131739377975\n",
      "Loss: 0.01879577711224556\n",
      "Loss: 0.018761225044727325\n",
      "Loss: 0.018727242946624756\n",
      "Loss: 0.018692348152399063\n",
      "Loss: 0.018658697605133057\n",
      "Loss: 0.018623817712068558\n",
      "Loss: 0.018589798361063004\n",
      "Loss: 0.018555304035544395\n",
      "Loss: 0.018520886078476906\n",
      "Loss: 0.01848677359521389\n",
      "Loss: 0.018451979383826256\n",
      "Loss: 0.018418248742818832\n",
      "Loss: 0.018383344635367393\n",
      "Loss: 0.018349451944231987\n",
      "Loss: 0.018314816057682037\n",
      "Loss: 0.01828053966164589\n",
      "Loss: 0.018246296793222427\n",
      "Loss: 0.018211644142866135\n",
      "Loss: 0.01817776821553707\n",
      "Loss: 0.01814286969602108\n",
      "Loss: 0.018109114840626717\n",
      "Loss: 0.018074344843626022\n",
      "Loss: 0.018040208145976067\n",
      "Loss: 0.018005816265940666\n",
      "Loss: 0.017971305176615715\n",
      "Loss: 0.01793729141354561\n",
      "Loss: 0.017902398481965065\n",
      "Loss: 0.017868772149086\n",
      "Loss: 0.01783386431634426\n",
      "Loss: 0.017799871042370796\n",
      "Loss: 0.017765337601304054\n",
      "Loss: 0.017730968073010445\n",
      "Loss: 0.017696814611554146\n",
      "Loss: 0.017662061378359795\n",
      "Loss: 0.017628289759159088\n",
      "Loss: 0.0175933875143528\n",
      "Loss: 0.017559533938765526\n",
      "Loss: 0.01752486266195774\n",
      "Loss: 0.017490629106760025\n",
      "Loss: 0.017456334084272385\n",
      "Loss: 0.017421722412109375\n",
      "Loss: 0.017387809231877327\n",
      "Loss: 0.017352908849716187\n",
      "Loss: 0.017319198697805405\n",
      "Loss: 0.01728438213467598\n",
      "Loss: 0.017250288277864456\n",
      "Loss: 0.017215853556990623\n",
      "Loss: 0.017181387171149254\n",
      "Loss: 0.017147328704595566\n",
      "Loss: 0.017112482339143753\n",
      "Loss: 0.017078805714845657\n",
      "Loss: 0.017043905332684517\n",
      "Loss: 0.017009954899549484\n",
      "Loss: 0.01697538234293461\n",
      "Loss: 0.01694103702902794\n",
      "Loss: 0.01690686121582985\n",
      "Loss: 0.016872137784957886\n",
      "Loss: 0.016838330775499344\n",
      "Loss: 0.016803432255983353\n",
      "Loss: 0.016769614070653915\n",
      "Loss: 0.01673489809036255\n",
      "Loss: 0.016700709238648415\n",
      "Loss: 0.01666637882590294\n",
      "Loss: 0.016631800681352615\n",
      "Loss: 0.01659785583615303\n",
      "Loss: 0.016562949866056442\n",
      "Loss: 0.016529273241758347\n",
      "Loss: 0.016494428738951683\n",
      "Loss: 0.016460370272397995\n",
      "Loss: 0.016425900161266327\n",
      "Loss: 0.016391465440392494\n",
      "Loss: 0.01635737530887127\n",
      "Loss: 0.016322556883096695\n",
      "Loss: 0.01628885231912136\n",
      "Loss: 0.016253944486379623\n",
      "Loss: 0.016220029443502426\n",
      "Loss: 0.016185421496629715\n",
      "Loss: 0.016151126474142075\n",
      "Loss: 0.016116898506879807\n",
      "Loss: 0.016082221642136574\n",
      "Loss: 0.016048375517129898\n",
      "Loss: 0.01601347327232361\n",
      "Loss: 0.015979699790477753\n",
      "Loss: 0.01594495214521885\n",
      "Loss: 0.015910785645246506\n",
      "Loss: 0.015876421704888344\n",
      "Loss: 0.015841882675886154\n",
      "Loss: 0.015807893127202988\n",
      "Loss: 0.015772998332977295\n",
      "Loss: 0.015739355236291885\n",
      "Loss: 0.01570446975529194\n",
      "Loss: 0.015670450404286385\n",
      "Loss: 0.015635941177606583\n",
      "Loss: 0.015601545572280884\n",
      "Loss: 0.015567416325211525\n",
      "Loss: 0.015532639808952808\n",
      "Loss: 0.015498891472816467\n",
      "Loss: 0.015463987365365028\n",
      "Loss: 0.01543011236935854\n",
      "Loss: 0.01539546251296997\n",
      "Loss: 0.015361204743385315\n",
      "Loss: 0.015326937660574913\n",
      "Loss: 0.015292299911379814\n",
      "Loss: 0.015258421190083027\n",
      "Loss: 0.015223512426018715\n",
      "Loss: 0.015189772471785545\n",
      "Loss: 0.015154987573623657\n",
      "Loss: 0.01512086670845747\n",
      "Loss: 0.015086461789906025\n",
      "Loss: 0.015051963739097118\n",
      "Loss: 0.015017936937510967\n",
      "Loss: 0.014983056113123894\n",
      "Loss: 0.014949413016438484\n",
      "Loss: 0.01491450984030962\n",
      "Loss: 0.014880528673529625\n",
      "Loss: 0.014845984987914562\n",
      "Loss: 0.014811624772846699\n",
      "Loss: 0.014777451753616333\n",
      "Loss: 0.014742719009518623\n",
      "Loss: 0.014708934351801872\n",
      "Loss: 0.01467402745038271\n",
      "Loss: 0.014640195295214653\n",
      "Loss: 0.014605514705181122\n",
      "Loss: 0.014571284875273705\n",
      "Loss: 0.014536982402205467\n",
      "Loss: 0.014502379111945629\n",
      "Loss: 0.01446845568716526\n",
      "Loss: 0.01443355344235897\n",
      "Loss: 0.014399850741028786\n",
      "Loss: 0.014365026727318764\n",
      "Loss: 0.014330940321087837\n",
      "Loss: 0.01429650466889143\n",
      "Loss: 0.014262044802308083\n",
      "Loss: 0.014227977022528648\n",
      "Loss: 0.014193138107657433\n",
      "Loss: 0.014159453101456165\n",
      "Loss: 0.014124554581940174\n",
      "Loss: 0.014090606942772865\n",
      "Loss: 0.014056024141609669\n",
      "Loss: 0.014021704904735088\n",
      "Loss: 0.013987499289214611\n",
      "Loss: 0.013952797278761864\n",
      "Loss: 0.013918980956077576\n",
      "Loss: 0.013884072192013264\n",
      "Loss: 0.01385026890784502\n",
      "Loss: 0.013815549202263355\n",
      "Loss: 0.013781366869807243\n",
      "Loss: 0.013747021555900574\n",
      "Loss: 0.013712462969124317\n",
      "Loss: 0.013678496703505516\n",
      "Loss: 0.013643595390021801\n",
      "Loss: 0.013609932735562325\n",
      "Loss: 0.013575072400271893\n",
      "Loss: 0.013541030697524548\n",
      "Loss: 0.01350654661655426\n",
      "Loss: 0.013472122140228748\n",
      "Loss: 0.013438018038868904\n",
      "Loss: 0.013403216376900673\n",
      "Loss: 0.013369491323828697\n",
      "Loss: 0.013334590010344982\n",
      "Loss: 0.013300687074661255\n",
      "Loss: 0.0132660623639822\n",
      "Loss: 0.013231784105300903\n",
      "Loss: 0.013197538442909718\n",
      "Loss: 0.013162882998585701\n",
      "Loss: 0.01312901359051466\n",
      "Loss: 0.013094114139676094\n",
      "Loss: 0.013060351833701134\n",
      "Loss: 0.01302559394389391\n",
      "Loss: 0.012991437688469887\n",
      "Loss: 0.012957069091498852\n",
      "Loss: 0.012922537513077259\n",
      "Loss: 0.012888537719845772\n",
      "Loss: 0.01285363920032978\n",
      "Loss: 0.012820012867450714\n",
      "Loss: 0.012785108759999275\n",
      "Loss: 0.012751109898090363\n",
      "Loss: 0.012716586701571941\n",
      "Loss: 0.012682202272117138\n",
      "Loss: 0.012648063711822033\n",
      "Loss: 0.012613298371434212\n",
      "Loss: 0.012579533271491528\n",
      "Loss: 0.01254463754594326\n",
      "Loss: 0.012510770931839943\n",
      "Loss: 0.01247610803693533\n",
      "Loss: 0.012441864237189293\n",
      "Loss: 0.012407584115862846\n",
      "Loss: 0.012372957542538643\n",
      "Loss: 0.012339059263467789\n",
      "Loss: 0.012304151430726051\n",
      "Loss: 0.012270430102944374\n",
      "Loss: 0.012235632166266441\n",
      "Loss: 0.012201527133584023\n",
      "Loss: 0.012167108245193958\n",
      "Loss: 0.012132621370255947\n",
      "Loss: 0.012098582461476326\n",
      "Loss: 0.012063717469573021\n",
      "Loss: 0.012030057609081268\n",
      "Loss: 0.011995160952210426\n",
      "Loss: 0.011961189098656178\n",
      "Loss: 0.011926629580557346\n",
      "Loss: 0.011892282404005527\n",
      "Loss: 0.011858103796839714\n",
      "Loss: 0.01182338036596775\n",
      "Loss: 0.011789577081799507\n",
      "Loss: 0.011754677630960941\n",
      "Loss: 0.011720849201083183\n",
      "Loss: 0.011686149053275585\n",
      "Loss: 0.011651946231722832\n",
      "Loss: 0.011617625132203102\n",
      "Loss: 0.011583036743104458\n",
      "Loss: 0.011549100279808044\n",
      "Loss: 0.011514198035001755\n",
      "Loss: 0.011480512097477913\n",
      "Loss: 0.011445674113929272\n",
      "Loss: 0.011411604471504688\n",
      "Loss: 0.01137714646756649\n",
      "Loss: 0.011342699639499187\n",
      "Loss: 0.011308628134429455\n",
      "Loss: 0.011273792013525963\n",
      "Loss: 0.01124009769409895\n",
      "Loss: 0.01120519544929266\n",
      "Loss: 0.011171268299221992\n",
      "Loss: 0.011136669665575027\n",
      "Loss: 0.011102361604571342\n",
      "Loss: 0.011068146675825119\n",
      "Loss: 0.011033458635210991\n",
      "Loss: 0.010999620892107487\n",
      "Loss: 0.010964717715978622\n",
      "Loss: 0.010930929332971573\n",
      "Loss: 0.01089619193226099\n",
      "Loss: 0.010862023569643497\n",
      "Loss: 0.01082766242325306\n",
      "Loss: 0.010793117806315422\n",
      "Loss: 0.01075914315879345\n",
      "Loss: 0.010724237188696861\n",
      "Loss: 0.010690595954656601\n",
      "Loss: 0.010655724443495274\n",
      "Loss: 0.010621682740747929\n",
      "Loss: 0.010587191209197044\n",
      "Loss: 0.010552778840065002\n",
      "Loss: 0.010518664494156837\n",
      "Loss: 0.010483875870704651\n",
      "Loss: 0.010450141504406929\n",
      "Loss: 0.010415233671665192\n",
      "Loss: 0.01038134004920721\n",
      "Loss: 0.010346716269850731\n",
      "Loss: 0.010312443599104881\n",
      "Loss: 0.010278185829520226\n",
      "Loss: 0.01024353876709938\n",
      "Loss: 0.010209662839770317\n",
      "Loss: 0.010174764320254326\n",
      "Loss: 0.010141006670892239\n",
      "Loss: 0.010106234811246395\n",
      "Loss: 0.010072106495499611\n",
      "Loss: 0.010037708096206188\n",
      "Loss: 0.010003196075558662\n",
      "Loss: 0.009969188831746578\n",
      "Loss: 0.009934291243553162\n",
      "Loss: 0.009900657460093498\n",
      "Loss: 0.00986576173454523\n",
      "Loss: 0.009831766597926617\n",
      "Loss: 0.009797231294214725\n",
      "Loss: 0.009762861765921116\n",
      "Loss: 0.009728707373142242\n",
      "Loss: 0.009693953208625317\n",
      "Loss: 0.009660175070166588\n",
      "Loss: 0.00962528120726347\n",
      "Loss: 0.009591431356966496\n",
      "Loss: 0.009556754492223263\n",
      "Loss: 0.009522520937025547\n",
      "Loss: 0.009488226845860481\n",
      "Loss: 0.009453615173697472\n",
      "Loss: 0.009419701062142849\n",
      "Loss: 0.009384801611304283\n",
      "Loss: 0.009351085871458054\n",
      "Loss: 0.009316272102296352\n",
      "Loss: 0.009282185696065426\n",
      "Loss: 0.00924774818122387\n",
      "Loss: 0.0092132817953825\n",
      "Loss: 0.009179223328828812\n",
      "Loss: 0.009144370444118977\n",
      "Loss: 0.009110702201724052\n",
      "Loss: 0.009075800888240337\n",
      "Loss: 0.00904183741658926\n",
      "Loss: 0.009007277898490429\n",
      "Loss: 0.008972937241196632\n",
      "Loss: 0.008938746526837349\n",
      "Loss: 0.008904037065804005\n",
      "Loss: 0.008870227262377739\n",
      "Loss: 0.008835317566990852\n",
      "Loss: 0.008801509626209736\n",
      "Loss: 0.008766795508563519\n",
      "Loss: 0.008732601068913937\n",
      "Loss: 0.008698273450136185\n",
      "Loss: 0.00866369716823101\n",
      "Loss: 0.00862974114716053\n",
      "Loss: 0.008594844490289688\n",
      "Loss: 0.008561169728636742\n",
      "Loss: 0.008526316843926907\n",
      "Loss: 0.008492263033986092\n",
      "Loss: 0.008457791991531849\n",
      "Loss: 0.00842335820198059\n",
      "Loss: 0.00838927086442709\n",
      "Loss: 0.008354445919394493\n",
      "Loss: 0.008320735767483711\n",
      "Loss: 0.008285840973258018\n",
      "Loss: 0.008251926861703396\n",
      "Loss: 0.008217317052185535\n",
      "Loss: 0.00818302109837532\n",
      "Loss: 0.008148789405822754\n",
      "Loss: 0.00811411626636982\n",
      "Loss: 0.008080266416072845\n",
      "Loss: 0.008045373484492302\n",
      "Loss: 0.008011586964130402\n",
      "Loss: 0.007976839318871498\n",
      "Loss: 0.007942684926092625\n",
      "Loss: 0.007908310741186142\n",
      "Loss: 0.007873778231441975\n",
      "Loss: 0.007839785888791084\n",
      "Loss: 0.0078048864379525185\n",
      "Loss: 0.007771248463541269\n",
      "Loss: 0.007736357860267162\n",
      "Loss: 0.00770234689116478\n",
      "Loss: 0.007667834404855967\n",
      "Loss: 0.007633437868207693\n",
      "Loss: 0.007599309086799622\n",
      "Loss: 0.007564532104879618\n",
      "Loss: 0.0075307851657271385\n",
      "Loss: 0.007495882920920849\n",
      "Loss: 0.007462003733962774\n",
      "Loss: 0.00742735480889678\n",
      "Loss: 0.007393099367618561\n",
      "Loss: 0.00735883554443717\n",
      "Loss: 0.007324191741645336\n",
      "Loss: 0.007290305104106665\n",
      "Loss: 0.007255404256284237\n",
      "Loss: 0.0072216675616800785\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(42)\n",
    "\n",
    "# 1 epoch is one loop through the data\n",
    "epochs = 1500 # Actually converges around 1750\n",
    "\n",
    "### Training\n",
    "# 0. Loop through the data\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode\n",
    "    model_0.train() # sets all parameters with require_grad=True to require gradients\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_pred = model_0.forward(X_train)\n",
    "\n",
    "    # 2. Calculate the loss\n",
    "    loss = Loss_fn(y_pred, y_train)\n",
    "    print(f\"Loss: {loss}\")\n",
    "\n",
    "    # 3. Optimizer zero grad, clear old gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Backpropagation on the loss wrt the parameters of the model, compute new gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer (perform gradient descent), update parameters\n",
    "    optimizer.step() # by default, how the optimizer changes will accumulate through the loop, so zero them at the start of the next iteration (step 3)\n",
    "\n",
    "    # Save to results\n",
    "    params = list(model_0.parameters())\n",
    "    results.append((params[0].item(), params[1].item()))\n",
    "    ### Testing\n",
    "    # model_0.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5a4dc455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results (weight, bias):\n",
      "Step 0: weight=0.3367, bias=0.1288\n",
      "Step 1: weight=0.3371, bias=0.1298\n",
      "Step 2: weight=0.3375, bias=0.1308\n",
      "Step 3: weight=0.3379, bias=0.1318\n",
      "Step 4: weight=0.3383, bias=0.1328\n",
      "Step 5: weight=0.3386, bias=0.1338\n",
      "Step 6: weight=0.3390, bias=0.1348\n",
      "Step 7: weight=0.3394, bias=0.1358\n",
      "Step 8: weight=0.3398, bias=0.1368\n",
      "Step 9: weight=0.3402, bias=0.1378\n",
      "Step 10: weight=0.3406, bias=0.1388\n",
      "Step 11: weight=0.3410, bias=0.1398\n",
      "Step 12: weight=0.3414, bias=0.1408\n",
      "Step 13: weight=0.3418, bias=0.1418\n",
      "Step 14: weight=0.3422, bias=0.1428\n",
      "Step 15: weight=0.3425, bias=0.1438\n",
      "Step 16: weight=0.3429, bias=0.1448\n",
      "Step 17: weight=0.3433, bias=0.1458\n",
      "Step 18: weight=0.3437, bias=0.1468\n",
      "Step 19: weight=0.3441, bias=0.1478\n",
      "Step 20: weight=0.3445, bias=0.1488\n",
      "Step 21: weight=0.3449, bias=0.1498\n",
      "Step 22: weight=0.3453, bias=0.1508\n",
      "Step 23: weight=0.3457, bias=0.1518\n",
      "Step 24: weight=0.3461, bias=0.1528\n",
      "Step 25: weight=0.3464, bias=0.1538\n",
      "Step 26: weight=0.3468, bias=0.1548\n",
      "Step 27: weight=0.3472, bias=0.1558\n",
      "Step 28: weight=0.3476, bias=0.1568\n",
      "Step 29: weight=0.3480, bias=0.1578\n",
      "Step 30: weight=0.3484, bias=0.1588\n",
      "Step 31: weight=0.3488, bias=0.1598\n",
      "Step 32: weight=0.3492, bias=0.1608\n",
      "Step 33: weight=0.3496, bias=0.1618\n",
      "Step 34: weight=0.3500, bias=0.1628\n",
      "Step 35: weight=0.3503, bias=0.1638\n",
      "Step 36: weight=0.3507, bias=0.1648\n",
      "Step 37: weight=0.3511, bias=0.1658\n",
      "Step 38: weight=0.3515, bias=0.1668\n",
      "Step 39: weight=0.3519, bias=0.1678\n",
      "Step 40: weight=0.3523, bias=0.1688\n",
      "Step 41: weight=0.3527, bias=0.1698\n",
      "Step 42: weight=0.3531, bias=0.1708\n",
      "Step 43: weight=0.3535, bias=0.1718\n",
      "Step 44: weight=0.3539, bias=0.1728\n",
      "Step 45: weight=0.3542, bias=0.1738\n",
      "Step 46: weight=0.3546, bias=0.1748\n",
      "Step 47: weight=0.3550, bias=0.1758\n",
      "Step 48: weight=0.3554, bias=0.1768\n",
      "Step 49: weight=0.3558, bias=0.1778\n",
      "Step 50: weight=0.3562, bias=0.1788\n",
      "Step 51: weight=0.3566, bias=0.1798\n",
      "Step 52: weight=0.3570, bias=0.1808\n",
      "Step 53: weight=0.3574, bias=0.1818\n",
      "Step 54: weight=0.3577, bias=0.1828\n",
      "Step 55: weight=0.3581, bias=0.1838\n",
      "Step 56: weight=0.3585, bias=0.1848\n",
      "Step 57: weight=0.3589, bias=0.1858\n",
      "Step 58: weight=0.3593, bias=0.1868\n",
      "Step 59: weight=0.3597, bias=0.1878\n",
      "Step 60: weight=0.3601, bias=0.1888\n",
      "Step 61: weight=0.3605, bias=0.1898\n",
      "Step 62: weight=0.3609, bias=0.1908\n",
      "Step 63: weight=0.3613, bias=0.1918\n",
      "Step 64: weight=0.3616, bias=0.1928\n",
      "Step 65: weight=0.3620, bias=0.1938\n",
      "Step 66: weight=0.3624, bias=0.1948\n",
      "Step 67: weight=0.3628, bias=0.1958\n",
      "Step 68: weight=0.3632, bias=0.1968\n",
      "Step 69: weight=0.3636, bias=0.1978\n",
      "Step 70: weight=0.3640, bias=0.1988\n",
      "Step 71: weight=0.3644, bias=0.1998\n",
      "Step 72: weight=0.3648, bias=0.2008\n",
      "Step 73: weight=0.3652, bias=0.2018\n",
      "Step 74: weight=0.3655, bias=0.2028\n",
      "Step 75: weight=0.3659, bias=0.2038\n",
      "Step 76: weight=0.3663, bias=0.2048\n",
      "Step 77: weight=0.3667, bias=0.2058\n",
      "Step 78: weight=0.3671, bias=0.2068\n",
      "Step 79: weight=0.3675, bias=0.2078\n",
      "Step 80: weight=0.3679, bias=0.2088\n",
      "Step 81: weight=0.3683, bias=0.2098\n",
      "Step 82: weight=0.3687, bias=0.2108\n",
      "Step 83: weight=0.3691, bias=0.2118\n",
      "Step 84: weight=0.3694, bias=0.2128\n",
      "Step 85: weight=0.3698, bias=0.2138\n",
      "Step 86: weight=0.3702, bias=0.2148\n",
      "Step 87: weight=0.3706, bias=0.2158\n",
      "Step 88: weight=0.3710, bias=0.2168\n",
      "Step 89: weight=0.3714, bias=0.2178\n",
      "Step 90: weight=0.3718, bias=0.2188\n",
      "Step 91: weight=0.3722, bias=0.2198\n",
      "Step 92: weight=0.3726, bias=0.2208\n",
      "Step 93: weight=0.3730, bias=0.2218\n",
      "Step 94: weight=0.3733, bias=0.2228\n",
      "Step 95: weight=0.3737, bias=0.2238\n",
      "Step 96: weight=0.3741, bias=0.2248\n",
      "Step 97: weight=0.3745, bias=0.2258\n",
      "Step 98: weight=0.3749, bias=0.2268\n",
      "Step 99: weight=0.3753, bias=0.2278\n",
      "Step 100: weight=0.3757, bias=0.2288\n",
      "Step 101: weight=0.3761, bias=0.2298\n",
      "Step 102: weight=0.3765, bias=0.2308\n",
      "Step 103: weight=0.3769, bias=0.2318\n",
      "Step 104: weight=0.3772, bias=0.2328\n",
      "Step 105: weight=0.3776, bias=0.2338\n",
      "Step 106: weight=0.3780, bias=0.2348\n",
      "Step 107: weight=0.3784, bias=0.2358\n",
      "Step 108: weight=0.3788, bias=0.2368\n",
      "Step 109: weight=0.3792, bias=0.2378\n",
      "Step 110: weight=0.3796, bias=0.2388\n",
      "Step 111: weight=0.3800, bias=0.2398\n",
      "Step 112: weight=0.3804, bias=0.2408\n",
      "Step 113: weight=0.3808, bias=0.2418\n",
      "Step 114: weight=0.3811, bias=0.2428\n",
      "Step 115: weight=0.3815, bias=0.2438\n",
      "Step 116: weight=0.3819, bias=0.2448\n",
      "Step 117: weight=0.3823, bias=0.2458\n",
      "Step 118: weight=0.3827, bias=0.2468\n",
      "Step 119: weight=0.3831, bias=0.2478\n",
      "Step 120: weight=0.3835, bias=0.2488\n",
      "Step 121: weight=0.3839, bias=0.2498\n",
      "Step 122: weight=0.3843, bias=0.2508\n",
      "Step 123: weight=0.3847, bias=0.2518\n",
      "Step 124: weight=0.3850, bias=0.2528\n",
      "Step 125: weight=0.3854, bias=0.2538\n",
      "Step 126: weight=0.3858, bias=0.2548\n",
      "Step 127: weight=0.3862, bias=0.2558\n",
      "Step 128: weight=0.3866, bias=0.2568\n",
      "Step 129: weight=0.3870, bias=0.2578\n",
      "Step 130: weight=0.3874, bias=0.2588\n",
      "Step 131: weight=0.3878, bias=0.2598\n",
      "Step 132: weight=0.3882, bias=0.2608\n",
      "Step 133: weight=0.3886, bias=0.2618\n",
      "Step 134: weight=0.3889, bias=0.2628\n",
      "Step 135: weight=0.3893, bias=0.2638\n",
      "Step 136: weight=0.3897, bias=0.2648\n",
      "Step 137: weight=0.3901, bias=0.2658\n",
      "Step 138: weight=0.3905, bias=0.2668\n",
      "Step 139: weight=0.3909, bias=0.2678\n",
      "Step 140: weight=0.3913, bias=0.2688\n",
      "Step 141: weight=0.3917, bias=0.2698\n",
      "Step 142: weight=0.3921, bias=0.2708\n",
      "Step 143: weight=0.3925, bias=0.2718\n",
      "Step 144: weight=0.3928, bias=0.2728\n",
      "Step 145: weight=0.3932, bias=0.2738\n",
      "Step 146: weight=0.3936, bias=0.2748\n",
      "Step 147: weight=0.3940, bias=0.2758\n",
      "Step 148: weight=0.3944, bias=0.2768\n",
      "Step 149: weight=0.3948, bias=0.2778\n",
      "Step 150: weight=0.3952, bias=0.2788\n",
      "Step 151: weight=0.3956, bias=0.2798\n",
      "Step 152: weight=0.3960, bias=0.2808\n",
      "Step 153: weight=0.3964, bias=0.2818\n",
      "Step 154: weight=0.3967, bias=0.2828\n",
      "Step 155: weight=0.3971, bias=0.2838\n",
      "Step 156: weight=0.3975, bias=0.2848\n",
      "Step 157: weight=0.3979, bias=0.2858\n",
      "Step 158: weight=0.3983, bias=0.2868\n",
      "Step 159: weight=0.3987, bias=0.2878\n",
      "Step 160: weight=0.3991, bias=0.2888\n",
      "Step 161: weight=0.3995, bias=0.2898\n",
      "Step 162: weight=0.3999, bias=0.2908\n",
      "Step 163: weight=0.4003, bias=0.2918\n",
      "Step 164: weight=0.4006, bias=0.2928\n",
      "Step 165: weight=0.4010, bias=0.2938\n",
      "Step 166: weight=0.4014, bias=0.2948\n",
      "Step 167: weight=0.4018, bias=0.2958\n",
      "Step 168: weight=0.4022, bias=0.2968\n",
      "Step 169: weight=0.4026, bias=0.2978\n",
      "Step 170: weight=0.4030, bias=0.2988\n",
      "Step 171: weight=0.4034, bias=0.2998\n",
      "Step 172: weight=0.4038, bias=0.3008\n",
      "Step 173: weight=0.4042, bias=0.3018\n",
      "Step 174: weight=0.4045, bias=0.3027\n",
      "Step 175: weight=0.4049, bias=0.3037\n",
      "Step 176: weight=0.4053, bias=0.3046\n",
      "Step 177: weight=0.4057, bias=0.3056\n",
      "Step 178: weight=0.4061, bias=0.3065\n",
      "Step 179: weight=0.4065, bias=0.3074\n",
      "Step 180: weight=0.4069, bias=0.3083\n",
      "Step 181: weight=0.4073, bias=0.3092\n",
      "Step 182: weight=0.4077, bias=0.3101\n",
      "Step 183: weight=0.4081, bias=0.3110\n",
      "Step 184: weight=0.4084, bias=0.3119\n",
      "Step 185: weight=0.4088, bias=0.3128\n",
      "Step 186: weight=0.4092, bias=0.3136\n",
      "Step 187: weight=0.4096, bias=0.3145\n",
      "Step 188: weight=0.4100, bias=0.3153\n",
      "Step 189: weight=0.4104, bias=0.3162\n",
      "Step 190: weight=0.4108, bias=0.3170\n",
      "Step 191: weight=0.4112, bias=0.3179\n",
      "Step 192: weight=0.4115, bias=0.3187\n",
      "Step 193: weight=0.4119, bias=0.3195\n",
      "Step 194: weight=0.4123, bias=0.3203\n",
      "Step 195: weight=0.4127, bias=0.3211\n",
      "Step 196: weight=0.4131, bias=0.3219\n",
      "Step 197: weight=0.4135, bias=0.3227\n",
      "Step 198: weight=0.4138, bias=0.3235\n",
      "Step 199: weight=0.4142, bias=0.3242\n",
      "Step 200: weight=0.4146, bias=0.3250\n",
      "Step 201: weight=0.4150, bias=0.3257\n",
      "Step 202: weight=0.4154, bias=0.3265\n",
      "Step 203: weight=0.4157, bias=0.3272\n",
      "Step 204: weight=0.4161, bias=0.3280\n",
      "Step 205: weight=0.4165, bias=0.3287\n",
      "Step 206: weight=0.4169, bias=0.3294\n",
      "Step 207: weight=0.4173, bias=0.3301\n",
      "Step 208: weight=0.4176, bias=0.3308\n",
      "Step 209: weight=0.4180, bias=0.3315\n",
      "Step 210: weight=0.4184, bias=0.3322\n",
      "Step 211: weight=0.4188, bias=0.3329\n",
      "Step 212: weight=0.4191, bias=0.3336\n",
      "Step 213: weight=0.4195, bias=0.3343\n",
      "Step 214: weight=0.4199, bias=0.3350\n",
      "Step 215: weight=0.4202, bias=0.3356\n",
      "Step 216: weight=0.4206, bias=0.3363\n",
      "Step 217: weight=0.4210, bias=0.3369\n",
      "Step 218: weight=0.4213, bias=0.3376\n",
      "Step 219: weight=0.4217, bias=0.3382\n",
      "Step 220: weight=0.4221, bias=0.3389\n",
      "Step 221: weight=0.4225, bias=0.3395\n",
      "Step 222: weight=0.4228, bias=0.3401\n",
      "Step 223: weight=0.4232, bias=0.3407\n",
      "Step 224: weight=0.4235, bias=0.3413\n",
      "Step 225: weight=0.4239, bias=0.3419\n",
      "Step 226: weight=0.4243, bias=0.3425\n",
      "Step 227: weight=0.4246, bias=0.3431\n",
      "Step 228: weight=0.4250, bias=0.3437\n",
      "Step 229: weight=0.4253, bias=0.3443\n",
      "Step 230: weight=0.4257, bias=0.3449\n",
      "Step 231: weight=0.4261, bias=0.3454\n",
      "Step 232: weight=0.4264, bias=0.3460\n",
      "Step 233: weight=0.4268, bias=0.3465\n",
      "Step 234: weight=0.4271, bias=0.3471\n",
      "Step 235: weight=0.4275, bias=0.3476\n",
      "Step 236: weight=0.4278, bias=0.3482\n",
      "Step 237: weight=0.4282, bias=0.3487\n",
      "Step 238: weight=0.4285, bias=0.3493\n",
      "Step 239: weight=0.4289, bias=0.3498\n",
      "Step 240: weight=0.4292, bias=0.3503\n",
      "Step 241: weight=0.4296, bias=0.3508\n",
      "Step 242: weight=0.4299, bias=0.3513\n",
      "Step 243: weight=0.4303, bias=0.3518\n",
      "Step 244: weight=0.4306, bias=0.3523\n",
      "Step 245: weight=0.4309, bias=0.3528\n",
      "Step 246: weight=0.4313, bias=0.3533\n",
      "Step 247: weight=0.4316, bias=0.3538\n",
      "Step 248: weight=0.4320, bias=0.3542\n",
      "Step 249: weight=0.4323, bias=0.3547\n",
      "Step 250: weight=0.4326, bias=0.3551\n",
      "Step 251: weight=0.4330, bias=0.3556\n",
      "Step 252: weight=0.4333, bias=0.3560\n",
      "Step 253: weight=0.4336, bias=0.3565\n",
      "Step 254: weight=0.4340, bias=0.3569\n",
      "Step 255: weight=0.4343, bias=0.3574\n",
      "Step 256: weight=0.4347, bias=0.3578\n",
      "Step 257: weight=0.4350, bias=0.3583\n",
      "Step 258: weight=0.4353, bias=0.3587\n",
      "Step 259: weight=0.4356, bias=0.3591\n",
      "Step 260: weight=0.4360, bias=0.3595\n",
      "Step 261: weight=0.4363, bias=0.3599\n",
      "Step 262: weight=0.4366, bias=0.3603\n",
      "Step 263: weight=0.4369, bias=0.3607\n",
      "Step 264: weight=0.4373, bias=0.3611\n",
      "Step 265: weight=0.4376, bias=0.3615\n",
      "Step 266: weight=0.4379, bias=0.3619\n",
      "Step 267: weight=0.4382, bias=0.3623\n",
      "Step 268: weight=0.4386, bias=0.3627\n",
      "Step 269: weight=0.4389, bias=0.3631\n",
      "Step 270: weight=0.4392, bias=0.3635\n",
      "Step 271: weight=0.4395, bias=0.3638\n",
      "Step 272: weight=0.4398, bias=0.3642\n",
      "Step 273: weight=0.4401, bias=0.3645\n",
      "Step 274: weight=0.4404, bias=0.3649\n",
      "Step 275: weight=0.4408, bias=0.3652\n",
      "Step 276: weight=0.4411, bias=0.3656\n",
      "Step 277: weight=0.4414, bias=0.3659\n",
      "Step 278: weight=0.4417, bias=0.3663\n",
      "Step 279: weight=0.4420, bias=0.3666\n",
      "Step 280: weight=0.4423, bias=0.3670\n",
      "Step 281: weight=0.4426, bias=0.3673\n",
      "Step 282: weight=0.4429, bias=0.3676\n",
      "Step 283: weight=0.4432, bias=0.3679\n",
      "Step 284: weight=0.4435, bias=0.3682\n",
      "Step 285: weight=0.4438, bias=0.3685\n",
      "Step 286: weight=0.4441, bias=0.3688\n",
      "Step 287: weight=0.4444, bias=0.3691\n",
      "Step 288: weight=0.4447, bias=0.3694\n",
      "Step 289: weight=0.4450, bias=0.3697\n",
      "Step 290: weight=0.4453, bias=0.3700\n",
      "Step 291: weight=0.4456, bias=0.3703\n",
      "Step 292: weight=0.4459, bias=0.3706\n",
      "Step 293: weight=0.4462, bias=0.3709\n",
      "Step 294: weight=0.4465, bias=0.3712\n",
      "Step 295: weight=0.4468, bias=0.3715\n",
      "Step 296: weight=0.4471, bias=0.3717\n",
      "Step 297: weight=0.4474, bias=0.3720\n",
      "Step 298: weight=0.4477, bias=0.3722\n",
      "Step 299: weight=0.4479, bias=0.3725\n",
      "Step 300: weight=0.4482, bias=0.3727\n",
      "Step 301: weight=0.4485, bias=0.3730\n",
      "Step 302: weight=0.4488, bias=0.3732\n",
      "Step 303: weight=0.4491, bias=0.3735\n",
      "Step 304: weight=0.4494, bias=0.3737\n",
      "Step 305: weight=0.4497, bias=0.3740\n",
      "Step 306: weight=0.4499, bias=0.3742\n",
      "Step 307: weight=0.4502, bias=0.3745\n",
      "Step 308: weight=0.4505, bias=0.3747\n",
      "Step 309: weight=0.4508, bias=0.3750\n",
      "Step 310: weight=0.4511, bias=0.3752\n",
      "Step 311: weight=0.4513, bias=0.3754\n",
      "Step 312: weight=0.4516, bias=0.3756\n",
      "Step 313: weight=0.4519, bias=0.3758\n",
      "Step 314: weight=0.4521, bias=0.3760\n",
      "Step 315: weight=0.4524, bias=0.3762\n",
      "Step 316: weight=0.4527, bias=0.3764\n",
      "Step 317: weight=0.4530, bias=0.3766\n",
      "Step 318: weight=0.4532, bias=0.3768\n",
      "Step 319: weight=0.4535, bias=0.3770\n",
      "Step 320: weight=0.4538, bias=0.3772\n",
      "Step 321: weight=0.4540, bias=0.3774\n",
      "Step 322: weight=0.4543, bias=0.3776\n",
      "Step 323: weight=0.4546, bias=0.3778\n",
      "Step 324: weight=0.4548, bias=0.3780\n",
      "Step 325: weight=0.4551, bias=0.3782\n",
      "Step 326: weight=0.4554, bias=0.3784\n",
      "Step 327: weight=0.4556, bias=0.3785\n",
      "Step 328: weight=0.4559, bias=0.3787\n",
      "Step 329: weight=0.4561, bias=0.3788\n",
      "Step 330: weight=0.4564, bias=0.3790\n",
      "Step 331: weight=0.4567, bias=0.3791\n",
      "Step 332: weight=0.4569, bias=0.3793\n",
      "Step 333: weight=0.4572, bias=0.3794\n",
      "Step 334: weight=0.4574, bias=0.3796\n",
      "Step 335: weight=0.4577, bias=0.3797\n",
      "Step 336: weight=0.4579, bias=0.3799\n",
      "Step 337: weight=0.4582, bias=0.3800\n",
      "Step 338: weight=0.4584, bias=0.3802\n",
      "Step 339: weight=0.4587, bias=0.3803\n",
      "Step 340: weight=0.4589, bias=0.3805\n",
      "Step 341: weight=0.4592, bias=0.3806\n",
      "Step 342: weight=0.4594, bias=0.3808\n",
      "Step 343: weight=0.4597, bias=0.3809\n",
      "Step 344: weight=0.4600, bias=0.3811\n",
      "Step 345: weight=0.4602, bias=0.3812\n",
      "Step 346: weight=0.4605, bias=0.3814\n",
      "Step 347: weight=0.4607, bias=0.3815\n",
      "Step 348: weight=0.4610, bias=0.3816\n",
      "Step 349: weight=0.4612, bias=0.3817\n",
      "Step 350: weight=0.4614, bias=0.3818\n",
      "Step 351: weight=0.4617, bias=0.3819\n",
      "Step 352: weight=0.4619, bias=0.3820\n",
      "Step 353: weight=0.4621, bias=0.3821\n",
      "Step 354: weight=0.4624, bias=0.3822\n",
      "Step 355: weight=0.4626, bias=0.3823\n",
      "Step 356: weight=0.4629, bias=0.3824\n",
      "Step 357: weight=0.4631, bias=0.3825\n",
      "Step 358: weight=0.4633, bias=0.3826\n",
      "Step 359: weight=0.4636, bias=0.3827\n",
      "Step 360: weight=0.4638, bias=0.3828\n",
      "Step 361: weight=0.4640, bias=0.3829\n",
      "Step 362: weight=0.4643, bias=0.3830\n",
      "Step 363: weight=0.4645, bias=0.3831\n",
      "Step 364: weight=0.4647, bias=0.3832\n",
      "Step 365: weight=0.4650, bias=0.3833\n",
      "Step 366: weight=0.4652, bias=0.3834\n",
      "Step 367: weight=0.4655, bias=0.3835\n",
      "Step 368: weight=0.4657, bias=0.3836\n",
      "Step 369: weight=0.4659, bias=0.3837\n",
      "Step 370: weight=0.4662, bias=0.3838\n",
      "Step 371: weight=0.4664, bias=0.3839\n",
      "Step 372: weight=0.4666, bias=0.3840\n",
      "Step 373: weight=0.4669, bias=0.3841\n",
      "Step 374: weight=0.4671, bias=0.3841\n",
      "Step 375: weight=0.4673, bias=0.3842\n",
      "Step 376: weight=0.4675, bias=0.3842\n",
      "Step 377: weight=0.4677, bias=0.3843\n",
      "Step 378: weight=0.4680, bias=0.3843\n",
      "Step 379: weight=0.4682, bias=0.3844\n",
      "Step 380: weight=0.4684, bias=0.3844\n",
      "Step 381: weight=0.4686, bias=0.3845\n",
      "Step 382: weight=0.4688, bias=0.3845\n",
      "Step 383: weight=0.4691, bias=0.3846\n",
      "Step 384: weight=0.4693, bias=0.3846\n",
      "Step 385: weight=0.4695, bias=0.3847\n",
      "Step 386: weight=0.4697, bias=0.3847\n",
      "Step 387: weight=0.4699, bias=0.3848\n",
      "Step 388: weight=0.4701, bias=0.3848\n",
      "Step 389: weight=0.4704, bias=0.3849\n",
      "Step 390: weight=0.4706, bias=0.3849\n",
      "Step 391: weight=0.4708, bias=0.3850\n",
      "Step 392: weight=0.4710, bias=0.3850\n",
      "Step 393: weight=0.4712, bias=0.3851\n",
      "Step 394: weight=0.4715, bias=0.3851\n",
      "Step 395: weight=0.4717, bias=0.3852\n",
      "Step 396: weight=0.4719, bias=0.3852\n",
      "Step 397: weight=0.4721, bias=0.3853\n",
      "Step 398: weight=0.4723, bias=0.3853\n",
      "Step 399: weight=0.4726, bias=0.3854\n",
      "Step 400: weight=0.4728, bias=0.3854\n",
      "Step 401: weight=0.4730, bias=0.3855\n",
      "Step 402: weight=0.4732, bias=0.3855\n",
      "Step 403: weight=0.4734, bias=0.3856\n",
      "Step 404: weight=0.4737, bias=0.3856\n",
      "Step 405: weight=0.4739, bias=0.3857\n",
      "Step 406: weight=0.4741, bias=0.3857\n",
      "Step 407: weight=0.4743, bias=0.3858\n",
      "Step 408: weight=0.4745, bias=0.3858\n",
      "Step 409: weight=0.4747, bias=0.3858\n",
      "Step 410: weight=0.4749, bias=0.3858\n",
      "Step 411: weight=0.4751, bias=0.3858\n",
      "Step 412: weight=0.4753, bias=0.3858\n",
      "Step 413: weight=0.4755, bias=0.3858\n",
      "Step 414: weight=0.4757, bias=0.3858\n",
      "Step 415: weight=0.4759, bias=0.3858\n",
      "Step 416: weight=0.4761, bias=0.3858\n",
      "Step 417: weight=0.4763, bias=0.3858\n",
      "Step 418: weight=0.4765, bias=0.3858\n",
      "Step 419: weight=0.4767, bias=0.3858\n",
      "Step 420: weight=0.4769, bias=0.3858\n",
      "Step 421: weight=0.4771, bias=0.3858\n",
      "Step 422: weight=0.4773, bias=0.3858\n",
      "Step 423: weight=0.4775, bias=0.3858\n",
      "Step 424: weight=0.4777, bias=0.3858\n",
      "Step 425: weight=0.4779, bias=0.3858\n",
      "Step 426: weight=0.4781, bias=0.3858\n",
      "Step 427: weight=0.4783, bias=0.3858\n",
      "Step 428: weight=0.4785, bias=0.3858\n",
      "Step 429: weight=0.4787, bias=0.3858\n",
      "Step 430: weight=0.4789, bias=0.3858\n",
      "Step 431: weight=0.4791, bias=0.3858\n",
      "Step 432: weight=0.4793, bias=0.3858\n",
      "Step 433: weight=0.4795, bias=0.3858\n",
      "Step 434: weight=0.4797, bias=0.3858\n",
      "Step 435: weight=0.4799, bias=0.3858\n",
      "Step 436: weight=0.4801, bias=0.3858\n",
      "Step 437: weight=0.4803, bias=0.3858\n",
      "Step 438: weight=0.4805, bias=0.3858\n",
      "Step 439: weight=0.4807, bias=0.3858\n",
      "Step 440: weight=0.4809, bias=0.3858\n",
      "Step 441: weight=0.4811, bias=0.3858\n",
      "Step 442: weight=0.4813, bias=0.3858\n",
      "Step 443: weight=0.4815, bias=0.3858\n",
      "Step 444: weight=0.4817, bias=0.3858\n",
      "Step 445: weight=0.4819, bias=0.3858\n",
      "Step 446: weight=0.4821, bias=0.3858\n",
      "Step 447: weight=0.4823, bias=0.3858\n",
      "Step 448: weight=0.4825, bias=0.3858\n",
      "Step 449: weight=0.4827, bias=0.3858\n",
      "Step 450: weight=0.4829, bias=0.3858\n",
      "Step 451: weight=0.4831, bias=0.3858\n",
      "Step 452: weight=0.4833, bias=0.3858\n",
      "Step 453: weight=0.4835, bias=0.3858\n",
      "Step 454: weight=0.4837, bias=0.3858\n",
      "Step 455: weight=0.4839, bias=0.3858\n",
      "Step 456: weight=0.4841, bias=0.3858\n",
      "Step 457: weight=0.4843, bias=0.3858\n",
      "Step 458: weight=0.4845, bias=0.3858\n",
      "Step 459: weight=0.4847, bias=0.3858\n",
      "Step 460: weight=0.4849, bias=0.3858\n",
      "Step 461: weight=0.4851, bias=0.3858\n",
      "Step 462: weight=0.4853, bias=0.3858\n",
      "Step 463: weight=0.4855, bias=0.3858\n",
      "Step 464: weight=0.4857, bias=0.3858\n",
      "Step 465: weight=0.4859, bias=0.3857\n",
      "Step 466: weight=0.4861, bias=0.3857\n",
      "Step 467: weight=0.4862, bias=0.3856\n",
      "Step 468: weight=0.4864, bias=0.3856\n",
      "Step 469: weight=0.4866, bias=0.3855\n",
      "Step 470: weight=0.4868, bias=0.3855\n",
      "Step 471: weight=0.4870, bias=0.3854\n",
      "Step 472: weight=0.4871, bias=0.3854\n",
      "Step 473: weight=0.4873, bias=0.3853\n",
      "Step 474: weight=0.4875, bias=0.3853\n",
      "Step 475: weight=0.4877, bias=0.3852\n",
      "Step 476: weight=0.4879, bias=0.3852\n",
      "Step 477: weight=0.4880, bias=0.3851\n",
      "Step 478: weight=0.4882, bias=0.3851\n",
      "Step 479: weight=0.4884, bias=0.3850\n",
      "Step 480: weight=0.4886, bias=0.3850\n",
      "Step 481: weight=0.4888, bias=0.3849\n",
      "Step 482: weight=0.4889, bias=0.3849\n",
      "Step 483: weight=0.4891, bias=0.3848\n",
      "Step 484: weight=0.4893, bias=0.3848\n",
      "Step 485: weight=0.4895, bias=0.3847\n",
      "Step 486: weight=0.4897, bias=0.3847\n",
      "Step 487: weight=0.4898, bias=0.3846\n",
      "Step 488: weight=0.4900, bias=0.3846\n",
      "Step 489: weight=0.4902, bias=0.3845\n",
      "Step 490: weight=0.4904, bias=0.3845\n",
      "Step 491: weight=0.4906, bias=0.3844\n",
      "Step 492: weight=0.4907, bias=0.3844\n",
      "Step 493: weight=0.4909, bias=0.3843\n",
      "Step 494: weight=0.4911, bias=0.3843\n",
      "Step 495: weight=0.4913, bias=0.3842\n",
      "Step 496: weight=0.4915, bias=0.3842\n",
      "Step 497: weight=0.4916, bias=0.3841\n",
      "Step 498: weight=0.4918, bias=0.3841\n",
      "Step 499: weight=0.4920, bias=0.3840\n",
      "Step 500: weight=0.4922, bias=0.3840\n",
      "Step 501: weight=0.4924, bias=0.3839\n",
      "Step 502: weight=0.4925, bias=0.3839\n",
      "Step 503: weight=0.4927, bias=0.3838\n",
      "Step 504: weight=0.4929, bias=0.3838\n",
      "Step 505: weight=0.4931, bias=0.3837\n",
      "Step 506: weight=0.4933, bias=0.3837\n",
      "Step 507: weight=0.4934, bias=0.3836\n",
      "Step 508: weight=0.4936, bias=0.3836\n",
      "Step 509: weight=0.4938, bias=0.3835\n",
      "Step 510: weight=0.4940, bias=0.3835\n",
      "Step 511: weight=0.4942, bias=0.3834\n",
      "Step 512: weight=0.4943, bias=0.3834\n",
      "Step 513: weight=0.4945, bias=0.3833\n",
      "Step 514: weight=0.4947, bias=0.3833\n",
      "Step 515: weight=0.4949, bias=0.3832\n",
      "Step 516: weight=0.4951, bias=0.3832\n",
      "Step 517: weight=0.4952, bias=0.3831\n",
      "Step 518: weight=0.4954, bias=0.3831\n",
      "Step 519: weight=0.4956, bias=0.3830\n",
      "Step 520: weight=0.4958, bias=0.3830\n",
      "Step 521: weight=0.4960, bias=0.3829\n",
      "Step 522: weight=0.4961, bias=0.3829\n",
      "Step 523: weight=0.4963, bias=0.3828\n",
      "Step 524: weight=0.4965, bias=0.3828\n",
      "Step 525: weight=0.4967, bias=0.3827\n",
      "Step 526: weight=0.4969, bias=0.3827\n",
      "Step 527: weight=0.4970, bias=0.3826\n",
      "Step 528: weight=0.4972, bias=0.3826\n",
      "Step 529: weight=0.4974, bias=0.3825\n",
      "Step 530: weight=0.4976, bias=0.3825\n",
      "Step 531: weight=0.4978, bias=0.3824\n",
      "Step 532: weight=0.4979, bias=0.3824\n",
      "Step 533: weight=0.4981, bias=0.3823\n",
      "Step 534: weight=0.4983, bias=0.3823\n",
      "Step 535: weight=0.4985, bias=0.3822\n",
      "Step 536: weight=0.4987, bias=0.3822\n",
      "Step 537: weight=0.4988, bias=0.3821\n",
      "Step 538: weight=0.4990, bias=0.3821\n",
      "Step 539: weight=0.4992, bias=0.3820\n",
      "Step 540: weight=0.4994, bias=0.3820\n",
      "Step 541: weight=0.4996, bias=0.3819\n",
      "Step 542: weight=0.4997, bias=0.3819\n",
      "Step 543: weight=0.4999, bias=0.3818\n",
      "Step 544: weight=0.5001, bias=0.3818\n",
      "Step 545: weight=0.5003, bias=0.3817\n",
      "Step 546: weight=0.5005, bias=0.3817\n",
      "Step 547: weight=0.5006, bias=0.3816\n",
      "Step 548: weight=0.5008, bias=0.3816\n",
      "Step 549: weight=0.5010, bias=0.3815\n",
      "Step 550: weight=0.5012, bias=0.3815\n",
      "Step 551: weight=0.5014, bias=0.3814\n",
      "Step 552: weight=0.5015, bias=0.3814\n",
      "Step 553: weight=0.5017, bias=0.3813\n",
      "Step 554: weight=0.5019, bias=0.3813\n",
      "Step 555: weight=0.5021, bias=0.3812\n",
      "Step 556: weight=0.5023, bias=0.3812\n",
      "Step 557: weight=0.5024, bias=0.3811\n",
      "Step 558: weight=0.5026, bias=0.3811\n",
      "Step 559: weight=0.5028, bias=0.3810\n",
      "Step 560: weight=0.5030, bias=0.3810\n",
      "Step 561: weight=0.5032, bias=0.3809\n",
      "Step 562: weight=0.5033, bias=0.3809\n",
      "Step 563: weight=0.5035, bias=0.3808\n",
      "Step 564: weight=0.5037, bias=0.3808\n",
      "Step 565: weight=0.5039, bias=0.3807\n",
      "Step 566: weight=0.5041, bias=0.3807\n",
      "Step 567: weight=0.5042, bias=0.3806\n",
      "Step 568: weight=0.5044, bias=0.3806\n",
      "Step 569: weight=0.5046, bias=0.3805\n",
      "Step 570: weight=0.5048, bias=0.3805\n",
      "Step 571: weight=0.5050, bias=0.3804\n",
      "Step 572: weight=0.5051, bias=0.3804\n",
      "Step 573: weight=0.5053, bias=0.3803\n",
      "Step 574: weight=0.5055, bias=0.3803\n",
      "Step 575: weight=0.5057, bias=0.3802\n",
      "Step 576: weight=0.5059, bias=0.3802\n",
      "Step 577: weight=0.5060, bias=0.3801\n",
      "Step 578: weight=0.5062, bias=0.3801\n",
      "Step 579: weight=0.5064, bias=0.3800\n",
      "Step 580: weight=0.5066, bias=0.3800\n",
      "Step 581: weight=0.5068, bias=0.3799\n",
      "Step 582: weight=0.5069, bias=0.3799\n",
      "Step 583: weight=0.5071, bias=0.3798\n",
      "Step 584: weight=0.5073, bias=0.3798\n",
      "Step 585: weight=0.5075, bias=0.3797\n",
      "Step 586: weight=0.5077, bias=0.3797\n",
      "Step 587: weight=0.5078, bias=0.3796\n",
      "Step 588: weight=0.5080, bias=0.3796\n",
      "Step 589: weight=0.5082, bias=0.3795\n",
      "Step 590: weight=0.5084, bias=0.3795\n",
      "Step 591: weight=0.5086, bias=0.3794\n",
      "Step 592: weight=0.5087, bias=0.3794\n",
      "Step 593: weight=0.5089, bias=0.3793\n",
      "Step 594: weight=0.5091, bias=0.3793\n",
      "Step 595: weight=0.5093, bias=0.3792\n",
      "Step 596: weight=0.5095, bias=0.3792\n",
      "Step 597: weight=0.5096, bias=0.3791\n",
      "Step 598: weight=0.5098, bias=0.3791\n",
      "Step 599: weight=0.5100, bias=0.3790\n",
      "Step 600: weight=0.5102, bias=0.3790\n",
      "Step 601: weight=0.5104, bias=0.3789\n",
      "Step 602: weight=0.5105, bias=0.3789\n",
      "Step 603: weight=0.5107, bias=0.3788\n",
      "Step 604: weight=0.5109, bias=0.3788\n",
      "Step 605: weight=0.5111, bias=0.3787\n",
      "Step 606: weight=0.5113, bias=0.3787\n",
      "Step 607: weight=0.5114, bias=0.3786\n",
      "Step 608: weight=0.5116, bias=0.3786\n",
      "Step 609: weight=0.5118, bias=0.3785\n",
      "Step 610: weight=0.5120, bias=0.3785\n",
      "Step 611: weight=0.5122, bias=0.3784\n",
      "Step 612: weight=0.5123, bias=0.3784\n",
      "Step 613: weight=0.5125, bias=0.3783\n",
      "Step 614: weight=0.5127, bias=0.3783\n",
      "Step 615: weight=0.5129, bias=0.3782\n",
      "Step 616: weight=0.5131, bias=0.3782\n",
      "Step 617: weight=0.5132, bias=0.3781\n",
      "Step 618: weight=0.5134, bias=0.3781\n",
      "Step 619: weight=0.5136, bias=0.3780\n",
      "Step 620: weight=0.5138, bias=0.3780\n",
      "Step 621: weight=0.5140, bias=0.3779\n",
      "Step 622: weight=0.5141, bias=0.3779\n",
      "Step 623: weight=0.5143, bias=0.3778\n",
      "Step 624: weight=0.5145, bias=0.3778\n",
      "Step 625: weight=0.5147, bias=0.3777\n",
      "Step 626: weight=0.5149, bias=0.3777\n",
      "Step 627: weight=0.5150, bias=0.3776\n",
      "Step 628: weight=0.5152, bias=0.3776\n",
      "Step 629: weight=0.5154, bias=0.3775\n",
      "Step 630: weight=0.5156, bias=0.3775\n",
      "Step 631: weight=0.5157, bias=0.3774\n",
      "Step 632: weight=0.5159, bias=0.3773\n",
      "Step 633: weight=0.5161, bias=0.3773\n",
      "Step 634: weight=0.5163, bias=0.3772\n",
      "Step 635: weight=0.5164, bias=0.3771\n",
      "Step 636: weight=0.5166, bias=0.3770\n",
      "Step 637: weight=0.5168, bias=0.3770\n",
      "Step 638: weight=0.5169, bias=0.3769\n",
      "Step 639: weight=0.5171, bias=0.3768\n",
      "Step 640: weight=0.5173, bias=0.3768\n",
      "Step 641: weight=0.5175, bias=0.3767\n",
      "Step 642: weight=0.5176, bias=0.3766\n",
      "Step 643: weight=0.5178, bias=0.3765\n",
      "Step 644: weight=0.5180, bias=0.3765\n",
      "Step 645: weight=0.5181, bias=0.3764\n",
      "Step 646: weight=0.5183, bias=0.3763\n",
      "Step 647: weight=0.5185, bias=0.3762\n",
      "Step 648: weight=0.5187, bias=0.3762\n",
      "Step 649: weight=0.5188, bias=0.3761\n",
      "Step 650: weight=0.5190, bias=0.3760\n",
      "Step 651: weight=0.5192, bias=0.3760\n",
      "Step 652: weight=0.5193, bias=0.3759\n",
      "Step 653: weight=0.5195, bias=0.3758\n",
      "Step 654: weight=0.5197, bias=0.3757\n",
      "Step 655: weight=0.5199, bias=0.3757\n",
      "Step 656: weight=0.5200, bias=0.3756\n",
      "Step 657: weight=0.5202, bias=0.3755\n",
      "Step 658: weight=0.5204, bias=0.3755\n",
      "Step 659: weight=0.5205, bias=0.3754\n",
      "Step 660: weight=0.5207, bias=0.3753\n",
      "Step 661: weight=0.5209, bias=0.3752\n",
      "Step 662: weight=0.5211, bias=0.3752\n",
      "Step 663: weight=0.5212, bias=0.3751\n",
      "Step 664: weight=0.5214, bias=0.3750\n",
      "Step 665: weight=0.5216, bias=0.3750\n",
      "Step 666: weight=0.5217, bias=0.3749\n",
      "Step 667: weight=0.5219, bias=0.3748\n",
      "Step 668: weight=0.5221, bias=0.3747\n",
      "Step 669: weight=0.5223, bias=0.3747\n",
      "Step 670: weight=0.5224, bias=0.3746\n",
      "Step 671: weight=0.5226, bias=0.3745\n",
      "Step 672: weight=0.5228, bias=0.3745\n",
      "Step 673: weight=0.5229, bias=0.3744\n",
      "Step 674: weight=0.5231, bias=0.3743\n",
      "Step 675: weight=0.5233, bias=0.3742\n",
      "Step 676: weight=0.5234, bias=0.3742\n",
      "Step 677: weight=0.5236, bias=0.3741\n",
      "Step 678: weight=0.5238, bias=0.3740\n",
      "Step 679: weight=0.5240, bias=0.3740\n",
      "Step 680: weight=0.5241, bias=0.3739\n",
      "Step 681: weight=0.5243, bias=0.3738\n",
      "Step 682: weight=0.5245, bias=0.3737\n",
      "Step 683: weight=0.5246, bias=0.3737\n",
      "Step 684: weight=0.5248, bias=0.3736\n",
      "Step 685: weight=0.5250, bias=0.3735\n",
      "Step 686: weight=0.5251, bias=0.3734\n",
      "Step 687: weight=0.5253, bias=0.3734\n",
      "Step 688: weight=0.5255, bias=0.3733\n",
      "Step 689: weight=0.5257, bias=0.3732\n",
      "Step 690: weight=0.5258, bias=0.3732\n",
      "Step 691: weight=0.5260, bias=0.3731\n",
      "Step 692: weight=0.5262, bias=0.3730\n",
      "Step 693: weight=0.5263, bias=0.3729\n",
      "Step 694: weight=0.5265, bias=0.3729\n",
      "Step 695: weight=0.5267, bias=0.3728\n",
      "Step 696: weight=0.5269, bias=0.3727\n",
      "Step 697: weight=0.5270, bias=0.3727\n",
      "Step 698: weight=0.5272, bias=0.3726\n",
      "Step 699: weight=0.5274, bias=0.3725\n",
      "Step 700: weight=0.5275, bias=0.3724\n",
      "Step 701: weight=0.5277, bias=0.3724\n",
      "Step 702: weight=0.5279, bias=0.3723\n",
      "Step 703: weight=0.5281, bias=0.3722\n",
      "Step 704: weight=0.5282, bias=0.3722\n",
      "Step 705: weight=0.5284, bias=0.3721\n",
      "Step 706: weight=0.5286, bias=0.3720\n",
      "Step 707: weight=0.5287, bias=0.3719\n",
      "Step 708: weight=0.5289, bias=0.3719\n",
      "Step 709: weight=0.5291, bias=0.3718\n",
      "Step 710: weight=0.5293, bias=0.3717\n",
      "Step 711: weight=0.5294, bias=0.3717\n",
      "Step 712: weight=0.5296, bias=0.3716\n",
      "Step 713: weight=0.5298, bias=0.3715\n",
      "Step 714: weight=0.5299, bias=0.3714\n",
      "Step 715: weight=0.5301, bias=0.3714\n",
      "Step 716: weight=0.5303, bias=0.3713\n",
      "Step 717: weight=0.5305, bias=0.3712\n",
      "Step 718: weight=0.5306, bias=0.3712\n",
      "Step 719: weight=0.5308, bias=0.3711\n",
      "Step 720: weight=0.5310, bias=0.3710\n",
      "Step 721: weight=0.5311, bias=0.3709\n",
      "Step 722: weight=0.5313, bias=0.3709\n",
      "Step 723: weight=0.5315, bias=0.3708\n",
      "Step 724: weight=0.5316, bias=0.3707\n",
      "Step 725: weight=0.5318, bias=0.3706\n",
      "Step 726: weight=0.5320, bias=0.3706\n",
      "Step 727: weight=0.5322, bias=0.3705\n",
      "Step 728: weight=0.5323, bias=0.3704\n",
      "Step 729: weight=0.5325, bias=0.3704\n",
      "Step 730: weight=0.5327, bias=0.3703\n",
      "Step 731: weight=0.5328, bias=0.3702\n",
      "Step 732: weight=0.5330, bias=0.3701\n",
      "Step 733: weight=0.5332, bias=0.3701\n",
      "Step 734: weight=0.5334, bias=0.3700\n",
      "Step 735: weight=0.5335, bias=0.3699\n",
      "Step 736: weight=0.5337, bias=0.3699\n",
      "Step 737: weight=0.5339, bias=0.3698\n",
      "Step 738: weight=0.5340, bias=0.3697\n",
      "Step 739: weight=0.5342, bias=0.3696\n",
      "Step 740: weight=0.5344, bias=0.3696\n",
      "Step 741: weight=0.5346, bias=0.3695\n",
      "Step 742: weight=0.5347, bias=0.3694\n",
      "Step 743: weight=0.5349, bias=0.3694\n",
      "Step 744: weight=0.5351, bias=0.3693\n",
      "Step 745: weight=0.5352, bias=0.3692\n",
      "Step 746: weight=0.5354, bias=0.3691\n",
      "Step 747: weight=0.5356, bias=0.3691\n",
      "Step 748: weight=0.5357, bias=0.3690\n",
      "Step 749: weight=0.5359, bias=0.3689\n",
      "Step 750: weight=0.5361, bias=0.3689\n",
      "Step 751: weight=0.5363, bias=0.3688\n",
      "Step 752: weight=0.5364, bias=0.3687\n",
      "Step 753: weight=0.5366, bias=0.3686\n",
      "Step 754: weight=0.5368, bias=0.3686\n",
      "Step 755: weight=0.5369, bias=0.3685\n",
      "Step 756: weight=0.5371, bias=0.3684\n",
      "Step 757: weight=0.5373, bias=0.3684\n",
      "Step 758: weight=0.5375, bias=0.3683\n",
      "Step 759: weight=0.5376, bias=0.3682\n",
      "Step 760: weight=0.5378, bias=0.3681\n",
      "Step 761: weight=0.5380, bias=0.3681\n",
      "Step 762: weight=0.5381, bias=0.3680\n",
      "Step 763: weight=0.5383, bias=0.3679\n",
      "Step 764: weight=0.5385, bias=0.3679\n",
      "Step 765: weight=0.5387, bias=0.3678\n",
      "Step 766: weight=0.5388, bias=0.3677\n",
      "Step 767: weight=0.5390, bias=0.3676\n",
      "Step 768: weight=0.5392, bias=0.3676\n",
      "Step 769: weight=0.5393, bias=0.3675\n",
      "Step 770: weight=0.5395, bias=0.3674\n",
      "Step 771: weight=0.5397, bias=0.3673\n",
      "Step 772: weight=0.5398, bias=0.3673\n",
      "Step 773: weight=0.5400, bias=0.3672\n",
      "Step 774: weight=0.5402, bias=0.3671\n",
      "Step 775: weight=0.5404, bias=0.3671\n",
      "Step 776: weight=0.5405, bias=0.3670\n",
      "Step 777: weight=0.5407, bias=0.3669\n",
      "Step 778: weight=0.5409, bias=0.3668\n",
      "Step 779: weight=0.5410, bias=0.3668\n",
      "Step 780: weight=0.5412, bias=0.3667\n",
      "Step 781: weight=0.5414, bias=0.3666\n",
      "Step 782: weight=0.5416, bias=0.3666\n",
      "Step 783: weight=0.5417, bias=0.3665\n",
      "Step 784: weight=0.5419, bias=0.3664\n",
      "Step 785: weight=0.5421, bias=0.3663\n",
      "Step 786: weight=0.5422, bias=0.3663\n",
      "Step 787: weight=0.5424, bias=0.3662\n",
      "Step 788: weight=0.5426, bias=0.3661\n",
      "Step 789: weight=0.5428, bias=0.3661\n",
      "Step 790: weight=0.5429, bias=0.3660\n",
      "Step 791: weight=0.5431, bias=0.3659\n",
      "Step 792: weight=0.5433, bias=0.3658\n",
      "Step 793: weight=0.5434, bias=0.3658\n",
      "Step 794: weight=0.5436, bias=0.3657\n",
      "Step 795: weight=0.5438, bias=0.3656\n",
      "Step 796: weight=0.5440, bias=0.3656\n",
      "Step 797: weight=0.5441, bias=0.3655\n",
      "Step 798: weight=0.5443, bias=0.3654\n",
      "Step 799: weight=0.5445, bias=0.3653\n",
      "Step 800: weight=0.5446, bias=0.3653\n",
      "Step 801: weight=0.5448, bias=0.3652\n",
      "Step 802: weight=0.5450, bias=0.3651\n",
      "Step 803: weight=0.5452, bias=0.3651\n",
      "Step 804: weight=0.5453, bias=0.3650\n",
      "Step 805: weight=0.5455, bias=0.3649\n",
      "Step 806: weight=0.5457, bias=0.3648\n",
      "Step 807: weight=0.5458, bias=0.3648\n",
      "Step 808: weight=0.5460, bias=0.3647\n",
      "Step 809: weight=0.5462, bias=0.3646\n",
      "Step 810: weight=0.5463, bias=0.3645\n",
      "Step 811: weight=0.5465, bias=0.3645\n",
      "Step 812: weight=0.5467, bias=0.3644\n",
      "Step 813: weight=0.5469, bias=0.3643\n",
      "Step 814: weight=0.5470, bias=0.3643\n",
      "Step 815: weight=0.5472, bias=0.3642\n",
      "Step 816: weight=0.5474, bias=0.3641\n",
      "Step 817: weight=0.5475, bias=0.3640\n",
      "Step 818: weight=0.5477, bias=0.3640\n",
      "Step 819: weight=0.5479, bias=0.3639\n",
      "Step 820: weight=0.5480, bias=0.3638\n",
      "Step 821: weight=0.5482, bias=0.3638\n",
      "Step 822: weight=0.5484, bias=0.3637\n",
      "Step 823: weight=0.5486, bias=0.3636\n",
      "Step 824: weight=0.5487, bias=0.3635\n",
      "Step 825: weight=0.5489, bias=0.3635\n",
      "Step 826: weight=0.5491, bias=0.3634\n",
      "Step 827: weight=0.5492, bias=0.3633\n",
      "Step 828: weight=0.5494, bias=0.3633\n",
      "Step 829: weight=0.5496, bias=0.3632\n",
      "Step 830: weight=0.5498, bias=0.3631\n",
      "Step 831: weight=0.5499, bias=0.3630\n",
      "Step 832: weight=0.5501, bias=0.3630\n",
      "Step 833: weight=0.5503, bias=0.3629\n",
      "Step 834: weight=0.5504, bias=0.3628\n",
      "Step 835: weight=0.5506, bias=0.3628\n",
      "Step 836: weight=0.5508, bias=0.3627\n",
      "Step 837: weight=0.5510, bias=0.3626\n",
      "Step 838: weight=0.5511, bias=0.3625\n",
      "Step 839: weight=0.5513, bias=0.3625\n",
      "Step 840: weight=0.5515, bias=0.3624\n",
      "Step 841: weight=0.5516, bias=0.3623\n",
      "Step 842: weight=0.5518, bias=0.3623\n",
      "Step 843: weight=0.5520, bias=0.3622\n",
      "Step 844: weight=0.5522, bias=0.3621\n",
      "Step 845: weight=0.5523, bias=0.3620\n",
      "Step 846: weight=0.5525, bias=0.3620\n",
      "Step 847: weight=0.5527, bias=0.3619\n",
      "Step 848: weight=0.5528, bias=0.3618\n",
      "Step 849: weight=0.5530, bias=0.3618\n",
      "Step 850: weight=0.5532, bias=0.3617\n",
      "Step 851: weight=0.5534, bias=0.3616\n",
      "Step 852: weight=0.5535, bias=0.3615\n",
      "Step 853: weight=0.5537, bias=0.3615\n",
      "Step 854: weight=0.5539, bias=0.3614\n",
      "Step 855: weight=0.5540, bias=0.3613\n",
      "Step 856: weight=0.5542, bias=0.3612\n",
      "Step 857: weight=0.5544, bias=0.3612\n",
      "Step 858: weight=0.5546, bias=0.3611\n",
      "Step 859: weight=0.5547, bias=0.3610\n",
      "Step 860: weight=0.5549, bias=0.3610\n",
      "Step 861: weight=0.5551, bias=0.3609\n",
      "Step 862: weight=0.5552, bias=0.3608\n",
      "Step 863: weight=0.5554, bias=0.3607\n",
      "Step 864: weight=0.5556, bias=0.3607\n",
      "Step 865: weight=0.5557, bias=0.3606\n",
      "Step 866: weight=0.5559, bias=0.3605\n",
      "Step 867: weight=0.5561, bias=0.3605\n",
      "Step 868: weight=0.5562, bias=0.3604\n",
      "Step 869: weight=0.5564, bias=0.3603\n",
      "Step 870: weight=0.5566, bias=0.3602\n",
      "Step 871: weight=0.5568, bias=0.3602\n",
      "Step 872: weight=0.5569, bias=0.3601\n",
      "Step 873: weight=0.5571, bias=0.3600\n",
      "Step 874: weight=0.5573, bias=0.3600\n",
      "Step 875: weight=0.5574, bias=0.3599\n",
      "Step 876: weight=0.5576, bias=0.3598\n",
      "Step 877: weight=0.5578, bias=0.3597\n",
      "Step 878: weight=0.5580, bias=0.3597\n",
      "Step 879: weight=0.5581, bias=0.3596\n",
      "Step 880: weight=0.5583, bias=0.3595\n",
      "Step 881: weight=0.5585, bias=0.3595\n",
      "Step 882: weight=0.5586, bias=0.3594\n",
      "Step 883: weight=0.5588, bias=0.3593\n",
      "Step 884: weight=0.5590, bias=0.3592\n",
      "Step 885: weight=0.5592, bias=0.3592\n",
      "Step 886: weight=0.5593, bias=0.3591\n",
      "Step 887: weight=0.5595, bias=0.3590\n",
      "Step 888: weight=0.5597, bias=0.3590\n",
      "Step 889: weight=0.5598, bias=0.3589\n",
      "Step 890: weight=0.5600, bias=0.3588\n",
      "Step 891: weight=0.5602, bias=0.3587\n",
      "Step 892: weight=0.5604, bias=0.3587\n",
      "Step 893: weight=0.5605, bias=0.3586\n",
      "Step 894: weight=0.5607, bias=0.3585\n",
      "Step 895: weight=0.5609, bias=0.3584\n",
      "Step 896: weight=0.5610, bias=0.3584\n",
      "Step 897: weight=0.5612, bias=0.3583\n",
      "Step 898: weight=0.5614, bias=0.3582\n",
      "Step 899: weight=0.5616, bias=0.3582\n",
      "Step 900: weight=0.5617, bias=0.3581\n",
      "Step 901: weight=0.5619, bias=0.3580\n",
      "Step 902: weight=0.5621, bias=0.3579\n",
      "Step 903: weight=0.5622, bias=0.3579\n",
      "Step 904: weight=0.5624, bias=0.3578\n",
      "Step 905: weight=0.5626, bias=0.3577\n",
      "Step 906: weight=0.5628, bias=0.3577\n",
      "Step 907: weight=0.5629, bias=0.3576\n",
      "Step 908: weight=0.5631, bias=0.3575\n",
      "Step 909: weight=0.5633, bias=0.3574\n",
      "Step 910: weight=0.5634, bias=0.3574\n",
      "Step 911: weight=0.5636, bias=0.3573\n",
      "Step 912: weight=0.5638, bias=0.3572\n",
      "Step 913: weight=0.5639, bias=0.3572\n",
      "Step 914: weight=0.5641, bias=0.3571\n",
      "Step 915: weight=0.5643, bias=0.3570\n",
      "Step 916: weight=0.5644, bias=0.3569\n",
      "Step 917: weight=0.5646, bias=0.3569\n",
      "Step 918: weight=0.5648, bias=0.3568\n",
      "Step 919: weight=0.5650, bias=0.3567\n",
      "Step 920: weight=0.5651, bias=0.3567\n",
      "Step 921: weight=0.5653, bias=0.3566\n",
      "Step 922: weight=0.5655, bias=0.3565\n",
      "Step 923: weight=0.5656, bias=0.3564\n",
      "Step 924: weight=0.5658, bias=0.3564\n",
      "Step 925: weight=0.5660, bias=0.3563\n",
      "Step 926: weight=0.5662, bias=0.3562\n",
      "Step 927: weight=0.5663, bias=0.3562\n",
      "Step 928: weight=0.5665, bias=0.3561\n",
      "Step 929: weight=0.5667, bias=0.3560\n",
      "Step 930: weight=0.5668, bias=0.3559\n",
      "Step 931: weight=0.5670, bias=0.3559\n",
      "Step 932: weight=0.5672, bias=0.3558\n",
      "Step 933: weight=0.5674, bias=0.3557\n",
      "Step 934: weight=0.5675, bias=0.3557\n",
      "Step 935: weight=0.5677, bias=0.3556\n",
      "Step 936: weight=0.5679, bias=0.3555\n",
      "Step 937: weight=0.5680, bias=0.3554\n",
      "Step 938: weight=0.5682, bias=0.3554\n",
      "Step 939: weight=0.5684, bias=0.3553\n",
      "Step 940: weight=0.5686, bias=0.3552\n",
      "Step 941: weight=0.5687, bias=0.3551\n",
      "Step 942: weight=0.5689, bias=0.3551\n",
      "Step 943: weight=0.5691, bias=0.3550\n",
      "Step 944: weight=0.5692, bias=0.3549\n",
      "Step 945: weight=0.5694, bias=0.3549\n",
      "Step 946: weight=0.5696, bias=0.3548\n",
      "Step 947: weight=0.5698, bias=0.3547\n",
      "Step 948: weight=0.5699, bias=0.3546\n",
      "Step 949: weight=0.5701, bias=0.3546\n",
      "Step 950: weight=0.5703, bias=0.3545\n",
      "Step 951: weight=0.5704, bias=0.3544\n",
      "Step 952: weight=0.5706, bias=0.3544\n",
      "Step 953: weight=0.5708, bias=0.3543\n",
      "Step 954: weight=0.5710, bias=0.3542\n",
      "Step 955: weight=0.5711, bias=0.3541\n",
      "Step 956: weight=0.5713, bias=0.3541\n",
      "Step 957: weight=0.5715, bias=0.3540\n",
      "Step 958: weight=0.5716, bias=0.3539\n",
      "Step 959: weight=0.5718, bias=0.3539\n",
      "Step 960: weight=0.5720, bias=0.3538\n",
      "Step 961: weight=0.5721, bias=0.3537\n",
      "Step 962: weight=0.5723, bias=0.3536\n",
      "Step 963: weight=0.5725, bias=0.3536\n",
      "Step 964: weight=0.5726, bias=0.3535\n",
      "Step 965: weight=0.5728, bias=0.3534\n",
      "Step 966: weight=0.5730, bias=0.3534\n",
      "Step 967: weight=0.5732, bias=0.3533\n",
      "Step 968: weight=0.5733, bias=0.3532\n",
      "Step 969: weight=0.5735, bias=0.3531\n",
      "Step 970: weight=0.5737, bias=0.3531\n",
      "Step 971: weight=0.5738, bias=0.3530\n",
      "Step 972: weight=0.5740, bias=0.3529\n",
      "Step 973: weight=0.5742, bias=0.3529\n",
      "Step 974: weight=0.5744, bias=0.3528\n",
      "Step 975: weight=0.5745, bias=0.3527\n",
      "Step 976: weight=0.5747, bias=0.3526\n",
      "Step 977: weight=0.5749, bias=0.3526\n",
      "Step 978: weight=0.5750, bias=0.3525\n",
      "Step 979: weight=0.5752, bias=0.3524\n",
      "Step 980: weight=0.5754, bias=0.3523\n",
      "Step 981: weight=0.5756, bias=0.3523\n",
      "Step 982: weight=0.5757, bias=0.3522\n",
      "Step 983: weight=0.5759, bias=0.3521\n",
      "Step 984: weight=0.5761, bias=0.3521\n",
      "Step 985: weight=0.5762, bias=0.3520\n",
      "Step 986: weight=0.5764, bias=0.3519\n",
      "Step 987: weight=0.5766, bias=0.3518\n",
      "Step 988: weight=0.5768, bias=0.3518\n",
      "Step 989: weight=0.5769, bias=0.3517\n",
      "Step 990: weight=0.5771, bias=0.3516\n",
      "Step 991: weight=0.5773, bias=0.3516\n",
      "Step 992: weight=0.5774, bias=0.3515\n",
      "Step 993: weight=0.5776, bias=0.3514\n",
      "Step 994: weight=0.5778, bias=0.3513\n",
      "Step 995: weight=0.5780, bias=0.3513\n",
      "Step 996: weight=0.5781, bias=0.3512\n",
      "Step 997: weight=0.5783, bias=0.3511\n",
      "Step 998: weight=0.5785, bias=0.3511\n",
      "Step 999: weight=0.5786, bias=0.3510\n",
      "Step 1000: weight=0.5788, bias=0.3509\n",
      "Step 1001: weight=0.5790, bias=0.3508\n",
      "Step 1002: weight=0.5792, bias=0.3508\n",
      "Step 1003: weight=0.5793, bias=0.3507\n",
      "Step 1004: weight=0.5795, bias=0.3506\n",
      "Step 1005: weight=0.5797, bias=0.3506\n",
      "Step 1006: weight=0.5798, bias=0.3505\n",
      "Step 1007: weight=0.5800, bias=0.3504\n",
      "Step 1008: weight=0.5802, bias=0.3503\n",
      "Step 1009: weight=0.5803, bias=0.3503\n",
      "Step 1010: weight=0.5805, bias=0.3502\n",
      "Step 1011: weight=0.5807, bias=0.3501\n",
      "Step 1012: weight=0.5809, bias=0.3501\n",
      "Step 1013: weight=0.5810, bias=0.3500\n",
      "Step 1014: weight=0.5812, bias=0.3499\n",
      "Step 1015: weight=0.5814, bias=0.3498\n",
      "Step 1016: weight=0.5815, bias=0.3498\n",
      "Step 1017: weight=0.5817, bias=0.3497\n",
      "Step 1018: weight=0.5819, bias=0.3496\n",
      "Step 1019: weight=0.5821, bias=0.3496\n",
      "Step 1020: weight=0.5822, bias=0.3495\n",
      "Step 1021: weight=0.5824, bias=0.3494\n",
      "Step 1022: weight=0.5826, bias=0.3493\n",
      "Step 1023: weight=0.5827, bias=0.3493\n",
      "Step 1024: weight=0.5829, bias=0.3492\n",
      "Step 1025: weight=0.5831, bias=0.3491\n",
      "Step 1026: weight=0.5832, bias=0.3490\n",
      "Step 1027: weight=0.5834, bias=0.3490\n",
      "Step 1028: weight=0.5836, bias=0.3489\n",
      "Step 1029: weight=0.5838, bias=0.3488\n",
      "Step 1030: weight=0.5839, bias=0.3488\n",
      "Step 1031: weight=0.5841, bias=0.3487\n",
      "Step 1032: weight=0.5843, bias=0.3486\n",
      "Step 1033: weight=0.5844, bias=0.3485\n",
      "Step 1034: weight=0.5846, bias=0.3485\n",
      "Step 1035: weight=0.5848, bias=0.3484\n",
      "Step 1036: weight=0.5850, bias=0.3483\n",
      "Step 1037: weight=0.5851, bias=0.3483\n",
      "Step 1038: weight=0.5853, bias=0.3482\n",
      "Step 1039: weight=0.5855, bias=0.3481\n",
      "Step 1040: weight=0.5856, bias=0.3480\n",
      "Step 1041: weight=0.5858, bias=0.3480\n",
      "Step 1042: weight=0.5860, bias=0.3479\n",
      "Step 1043: weight=0.5862, bias=0.3478\n",
      "Step 1044: weight=0.5863, bias=0.3478\n",
      "Step 1045: weight=0.5865, bias=0.3477\n",
      "Step 1046: weight=0.5867, bias=0.3476\n",
      "Step 1047: weight=0.5868, bias=0.3475\n",
      "Step 1048: weight=0.5870, bias=0.3475\n",
      "Step 1049: weight=0.5872, bias=0.3474\n",
      "Step 1050: weight=0.5874, bias=0.3473\n",
      "Step 1051: weight=0.5875, bias=0.3473\n",
      "Step 1052: weight=0.5877, bias=0.3472\n",
      "Step 1053: weight=0.5879, bias=0.3471\n",
      "Step 1054: weight=0.5880, bias=0.3470\n",
      "Step 1055: weight=0.5882, bias=0.3470\n",
      "Step 1056: weight=0.5884, bias=0.3469\n",
      "Step 1057: weight=0.5885, bias=0.3468\n",
      "Step 1058: weight=0.5887, bias=0.3468\n",
      "Step 1059: weight=0.5889, bias=0.3467\n",
      "Step 1060: weight=0.5891, bias=0.3466\n",
      "Step 1061: weight=0.5892, bias=0.3465\n",
      "Step 1062: weight=0.5894, bias=0.3465\n",
      "Step 1063: weight=0.5896, bias=0.3464\n",
      "Step 1064: weight=0.5897, bias=0.3463\n",
      "Step 1065: weight=0.5899, bias=0.3462\n",
      "Step 1066: weight=0.5901, bias=0.3462\n",
      "Step 1067: weight=0.5903, bias=0.3461\n",
      "Step 1068: weight=0.5904, bias=0.3460\n",
      "Step 1069: weight=0.5906, bias=0.3460\n",
      "Step 1070: weight=0.5908, bias=0.3459\n",
      "Step 1071: weight=0.5909, bias=0.3458\n",
      "Step 1072: weight=0.5911, bias=0.3457\n",
      "Step 1073: weight=0.5913, bias=0.3457\n",
      "Step 1074: weight=0.5915, bias=0.3456\n",
      "Step 1075: weight=0.5916, bias=0.3455\n",
      "Step 1076: weight=0.5918, bias=0.3455\n",
      "Step 1077: weight=0.5920, bias=0.3454\n",
      "Step 1078: weight=0.5921, bias=0.3453\n",
      "Step 1079: weight=0.5923, bias=0.3452\n",
      "Step 1080: weight=0.5925, bias=0.3452\n",
      "Step 1081: weight=0.5927, bias=0.3451\n",
      "Step 1082: weight=0.5928, bias=0.3450\n",
      "Step 1083: weight=0.5930, bias=0.3450\n",
      "Step 1084: weight=0.5932, bias=0.3449\n",
      "Step 1085: weight=0.5933, bias=0.3448\n",
      "Step 1086: weight=0.5935, bias=0.3447\n",
      "Step 1087: weight=0.5937, bias=0.3447\n",
      "Step 1088: weight=0.5938, bias=0.3446\n",
      "Step 1089: weight=0.5940, bias=0.3445\n",
      "Step 1090: weight=0.5942, bias=0.3445\n",
      "Step 1091: weight=0.5944, bias=0.3444\n",
      "Step 1092: weight=0.5945, bias=0.3443\n",
      "Step 1093: weight=0.5947, bias=0.3442\n",
      "Step 1094: weight=0.5949, bias=0.3442\n",
      "Step 1095: weight=0.5950, bias=0.3441\n",
      "Step 1096: weight=0.5952, bias=0.3440\n",
      "Step 1097: weight=0.5954, bias=0.3440\n",
      "Step 1098: weight=0.5956, bias=0.3439\n",
      "Step 1099: weight=0.5957, bias=0.3438\n",
      "Step 1100: weight=0.5959, bias=0.3437\n",
      "Step 1101: weight=0.5961, bias=0.3437\n",
      "Step 1102: weight=0.5962, bias=0.3436\n",
      "Step 1103: weight=0.5964, bias=0.3435\n",
      "Step 1104: weight=0.5966, bias=0.3435\n",
      "Step 1105: weight=0.5967, bias=0.3434\n",
      "Step 1106: weight=0.5969, bias=0.3433\n",
      "Step 1107: weight=0.5971, bias=0.3432\n",
      "Step 1108: weight=0.5973, bias=0.3432\n",
      "Step 1109: weight=0.5974, bias=0.3431\n",
      "Step 1110: weight=0.5976, bias=0.3430\n",
      "Step 1111: weight=0.5978, bias=0.3429\n",
      "Step 1112: weight=0.5979, bias=0.3429\n",
      "Step 1113: weight=0.5981, bias=0.3428\n",
      "Step 1114: weight=0.5983, bias=0.3427\n",
      "Step 1115: weight=0.5985, bias=0.3427\n",
      "Step 1116: weight=0.5986, bias=0.3426\n",
      "Step 1117: weight=0.5988, bias=0.3425\n",
      "Step 1118: weight=0.5990, bias=0.3424\n",
      "Step 1119: weight=0.5991, bias=0.3424\n",
      "Step 1120: weight=0.5993, bias=0.3423\n",
      "Step 1121: weight=0.5995, bias=0.3422\n",
      "Step 1122: weight=0.5997, bias=0.3422\n",
      "Step 1123: weight=0.5998, bias=0.3421\n",
      "Step 1124: weight=0.6000, bias=0.3420\n",
      "Step 1125: weight=0.6002, bias=0.3419\n",
      "Step 1126: weight=0.6003, bias=0.3419\n",
      "Step 1127: weight=0.6005, bias=0.3418\n",
      "Step 1128: weight=0.6007, bias=0.3417\n",
      "Step 1129: weight=0.6009, bias=0.3417\n",
      "Step 1130: weight=0.6010, bias=0.3416\n",
      "Step 1131: weight=0.6012, bias=0.3415\n",
      "Step 1132: weight=0.6014, bias=0.3414\n",
      "Step 1133: weight=0.6015, bias=0.3414\n",
      "Step 1134: weight=0.6017, bias=0.3413\n",
      "Step 1135: weight=0.6019, bias=0.3412\n",
      "Step 1136: weight=0.6021, bias=0.3412\n",
      "Step 1137: weight=0.6022, bias=0.3411\n",
      "Step 1138: weight=0.6024, bias=0.3410\n",
      "Step 1139: weight=0.6026, bias=0.3409\n",
      "Step 1140: weight=0.6027, bias=0.3409\n",
      "Step 1141: weight=0.6029, bias=0.3408\n",
      "Step 1142: weight=0.6031, bias=0.3407\n",
      "Step 1143: weight=0.6033, bias=0.3407\n",
      "Step 1144: weight=0.6034, bias=0.3406\n",
      "Step 1145: weight=0.6036, bias=0.3405\n",
      "Step 1146: weight=0.6038, bias=0.3404\n",
      "Step 1147: weight=0.6039, bias=0.3404\n",
      "Step 1148: weight=0.6041, bias=0.3403\n",
      "Step 1149: weight=0.6043, bias=0.3402\n",
      "Step 1150: weight=0.6044, bias=0.3401\n",
      "Step 1151: weight=0.6046, bias=0.3401\n",
      "Step 1152: weight=0.6048, bias=0.3400\n",
      "Step 1153: weight=0.6049, bias=0.3399\n",
      "Step 1154: weight=0.6051, bias=0.3399\n",
      "Step 1155: weight=0.6053, bias=0.3398\n",
      "Step 1156: weight=0.6055, bias=0.3397\n",
      "Step 1157: weight=0.6056, bias=0.3396\n",
      "Step 1158: weight=0.6058, bias=0.3396\n",
      "Step 1159: weight=0.6060, bias=0.3395\n",
      "Step 1160: weight=0.6061, bias=0.3394\n",
      "Step 1161: weight=0.6063, bias=0.3394\n",
      "Step 1162: weight=0.6065, bias=0.3393\n",
      "Step 1163: weight=0.6067, bias=0.3392\n",
      "Step 1164: weight=0.6068, bias=0.3391\n",
      "Step 1165: weight=0.6070, bias=0.3391\n",
      "Step 1166: weight=0.6072, bias=0.3390\n",
      "Step 1167: weight=0.6073, bias=0.3389\n",
      "Step 1168: weight=0.6075, bias=0.3389\n",
      "Step 1169: weight=0.6077, bias=0.3388\n",
      "Step 1170: weight=0.6079, bias=0.3387\n",
      "Step 1171: weight=0.6080, bias=0.3386\n",
      "Step 1172: weight=0.6082, bias=0.3386\n",
      "Step 1173: weight=0.6084, bias=0.3385\n",
      "Step 1174: weight=0.6085, bias=0.3384\n",
      "Step 1175: weight=0.6087, bias=0.3384\n",
      "Step 1176: weight=0.6089, bias=0.3383\n",
      "Step 1177: weight=0.6091, bias=0.3382\n",
      "Step 1178: weight=0.6092, bias=0.3381\n",
      "Step 1179: weight=0.6094, bias=0.3381\n",
      "Step 1180: weight=0.6096, bias=0.3380\n",
      "Step 1181: weight=0.6097, bias=0.3379\n",
      "Step 1182: weight=0.6099, bias=0.3379\n",
      "Step 1183: weight=0.6101, bias=0.3378\n",
      "Step 1184: weight=0.6103, bias=0.3377\n",
      "Step 1185: weight=0.6104, bias=0.3376\n",
      "Step 1186: weight=0.6106, bias=0.3376\n",
      "Step 1187: weight=0.6108, bias=0.3375\n",
      "Step 1188: weight=0.6109, bias=0.3374\n",
      "Step 1189: weight=0.6111, bias=0.3373\n",
      "Step 1190: weight=0.6113, bias=0.3373\n",
      "Step 1191: weight=0.6115, bias=0.3372\n",
      "Step 1192: weight=0.6116, bias=0.3371\n",
      "Step 1193: weight=0.6118, bias=0.3371\n",
      "Step 1194: weight=0.6120, bias=0.3370\n",
      "Step 1195: weight=0.6121, bias=0.3369\n",
      "Step 1196: weight=0.6123, bias=0.3368\n",
      "Step 1197: weight=0.6125, bias=0.3368\n",
      "Step 1198: weight=0.6126, bias=0.3367\n",
      "Step 1199: weight=0.6128, bias=0.3366\n",
      "Step 1200: weight=0.6130, bias=0.3366\n",
      "Step 1201: weight=0.6131, bias=0.3365\n",
      "Step 1202: weight=0.6133, bias=0.3364\n",
      "Step 1203: weight=0.6135, bias=0.3363\n",
      "Step 1204: weight=0.6137, bias=0.3363\n",
      "Step 1205: weight=0.6138, bias=0.3362\n",
      "Step 1206: weight=0.6140, bias=0.3361\n",
      "Step 1207: weight=0.6142, bias=0.3361\n",
      "Step 1208: weight=0.6143, bias=0.3360\n",
      "Step 1209: weight=0.6145, bias=0.3359\n",
      "Step 1210: weight=0.6147, bias=0.3358\n",
      "Step 1211: weight=0.6149, bias=0.3358\n",
      "Step 1212: weight=0.6150, bias=0.3357\n",
      "Step 1213: weight=0.6152, bias=0.3356\n",
      "Step 1214: weight=0.6154, bias=0.3356\n",
      "Step 1215: weight=0.6155, bias=0.3355\n",
      "Step 1216: weight=0.6157, bias=0.3354\n",
      "Step 1217: weight=0.6159, bias=0.3353\n",
      "Step 1218: weight=0.6161, bias=0.3353\n",
      "Step 1219: weight=0.6162, bias=0.3352\n",
      "Step 1220: weight=0.6164, bias=0.3351\n",
      "Step 1221: weight=0.6166, bias=0.3351\n",
      "Step 1222: weight=0.6167, bias=0.3350\n",
      "Step 1223: weight=0.6169, bias=0.3349\n",
      "Step 1224: weight=0.6171, bias=0.3348\n",
      "Step 1225: weight=0.6173, bias=0.3348\n",
      "Step 1226: weight=0.6174, bias=0.3347\n",
      "Step 1227: weight=0.6176, bias=0.3346\n",
      "Step 1228: weight=0.6178, bias=0.3346\n",
      "Step 1229: weight=0.6179, bias=0.3345\n",
      "Step 1230: weight=0.6181, bias=0.3344\n",
      "Step 1231: weight=0.6183, bias=0.3343\n",
      "Step 1232: weight=0.6185, bias=0.3343\n",
      "Step 1233: weight=0.6186, bias=0.3342\n",
      "Step 1234: weight=0.6188, bias=0.3341\n",
      "Step 1235: weight=0.6190, bias=0.3340\n",
      "Step 1236: weight=0.6191, bias=0.3340\n",
      "Step 1237: weight=0.6193, bias=0.3339\n",
      "Step 1238: weight=0.6195, bias=0.3338\n",
      "Step 1239: weight=0.6197, bias=0.3338\n",
      "Step 1240: weight=0.6198, bias=0.3337\n",
      "Step 1241: weight=0.6200, bias=0.3336\n",
      "Step 1242: weight=0.6202, bias=0.3335\n",
      "Step 1243: weight=0.6203, bias=0.3335\n",
      "Step 1244: weight=0.6205, bias=0.3334\n",
      "Step 1245: weight=0.6207, bias=0.3333\n",
      "Step 1246: weight=0.6208, bias=0.3333\n",
      "Step 1247: weight=0.6210, bias=0.3332\n",
      "Step 1248: weight=0.6212, bias=0.3331\n",
      "Step 1249: weight=0.6213, bias=0.3330\n",
      "Step 1250: weight=0.6215, bias=0.3330\n",
      "Step 1251: weight=0.6217, bias=0.3329\n",
      "Step 1252: weight=0.6219, bias=0.3328\n",
      "Step 1253: weight=0.6220, bias=0.3328\n",
      "Step 1254: weight=0.6222, bias=0.3327\n",
      "Step 1255: weight=0.6224, bias=0.3326\n",
      "Step 1256: weight=0.6225, bias=0.3325\n",
      "Step 1257: weight=0.6227, bias=0.3325\n",
      "Step 1258: weight=0.6229, bias=0.3324\n",
      "Step 1259: weight=0.6231, bias=0.3323\n",
      "Step 1260: weight=0.6232, bias=0.3323\n",
      "Step 1261: weight=0.6234, bias=0.3322\n",
      "Step 1262: weight=0.6236, bias=0.3321\n",
      "Step 1263: weight=0.6237, bias=0.3320\n",
      "Step 1264: weight=0.6239, bias=0.3320\n",
      "Step 1265: weight=0.6241, bias=0.3319\n",
      "Step 1266: weight=0.6243, bias=0.3318\n",
      "Step 1267: weight=0.6244, bias=0.3318\n",
      "Step 1268: weight=0.6246, bias=0.3317\n",
      "Step 1269: weight=0.6248, bias=0.3316\n",
      "Step 1270: weight=0.6249, bias=0.3315\n",
      "Step 1271: weight=0.6251, bias=0.3315\n",
      "Step 1272: weight=0.6253, bias=0.3314\n",
      "Step 1273: weight=0.6255, bias=0.3313\n",
      "Step 1274: weight=0.6256, bias=0.3312\n",
      "Step 1275: weight=0.6258, bias=0.3312\n",
      "Step 1276: weight=0.6260, bias=0.3311\n",
      "Step 1277: weight=0.6261, bias=0.3310\n",
      "Step 1278: weight=0.6263, bias=0.3310\n",
      "Step 1279: weight=0.6265, bias=0.3309\n",
      "Step 1280: weight=0.6267, bias=0.3308\n",
      "Step 1281: weight=0.6268, bias=0.3307\n",
      "Step 1282: weight=0.6270, bias=0.3307\n",
      "Step 1283: weight=0.6272, bias=0.3306\n",
      "Step 1284: weight=0.6273, bias=0.3305\n",
      "Step 1285: weight=0.6275, bias=0.3305\n",
      "Step 1286: weight=0.6277, bias=0.3304\n",
      "Step 1287: weight=0.6279, bias=0.3303\n",
      "Step 1288: weight=0.6280, bias=0.3302\n",
      "Step 1289: weight=0.6282, bias=0.3302\n",
      "Step 1290: weight=0.6284, bias=0.3301\n",
      "Step 1291: weight=0.6285, bias=0.3300\n",
      "Step 1292: weight=0.6287, bias=0.3300\n",
      "Step 1293: weight=0.6289, bias=0.3299\n",
      "Step 1294: weight=0.6290, bias=0.3298\n",
      "Step 1295: weight=0.6292, bias=0.3297\n",
      "Step 1296: weight=0.6294, bias=0.3297\n",
      "Step 1297: weight=0.6295, bias=0.3296\n",
      "Step 1298: weight=0.6297, bias=0.3295\n",
      "Step 1299: weight=0.6299, bias=0.3295\n",
      "Step 1300: weight=0.6301, bias=0.3294\n",
      "Step 1301: weight=0.6302, bias=0.3293\n",
      "Step 1302: weight=0.6304, bias=0.3292\n",
      "Step 1303: weight=0.6306, bias=0.3292\n",
      "Step 1304: weight=0.6307, bias=0.3291\n",
      "Step 1305: weight=0.6309, bias=0.3290\n",
      "Step 1306: weight=0.6311, bias=0.3290\n",
      "Step 1307: weight=0.6313, bias=0.3289\n",
      "Step 1308: weight=0.6314, bias=0.3288\n",
      "Step 1309: weight=0.6316, bias=0.3287\n",
      "Step 1310: weight=0.6318, bias=0.3287\n",
      "Step 1311: weight=0.6319, bias=0.3286\n",
      "Step 1312: weight=0.6321, bias=0.3285\n",
      "Step 1313: weight=0.6323, bias=0.3285\n",
      "Step 1314: weight=0.6325, bias=0.3284\n",
      "Step 1315: weight=0.6326, bias=0.3283\n",
      "Step 1316: weight=0.6328, bias=0.3282\n",
      "Step 1317: weight=0.6330, bias=0.3282\n",
      "Step 1318: weight=0.6331, bias=0.3281\n",
      "Step 1319: weight=0.6333, bias=0.3280\n",
      "Step 1320: weight=0.6335, bias=0.3279\n",
      "Step 1321: weight=0.6337, bias=0.3279\n",
      "Step 1322: weight=0.6338, bias=0.3278\n",
      "Step 1323: weight=0.6340, bias=0.3277\n",
      "Step 1324: weight=0.6342, bias=0.3277\n",
      "Step 1325: weight=0.6343, bias=0.3276\n",
      "Step 1326: weight=0.6345, bias=0.3275\n",
      "Step 1327: weight=0.6347, bias=0.3274\n",
      "Step 1328: weight=0.6349, bias=0.3274\n",
      "Step 1329: weight=0.6350, bias=0.3273\n",
      "Step 1330: weight=0.6352, bias=0.3272\n",
      "Step 1331: weight=0.6354, bias=0.3272\n",
      "Step 1332: weight=0.6355, bias=0.3271\n",
      "Step 1333: weight=0.6357, bias=0.3270\n",
      "Step 1334: weight=0.6359, bias=0.3269\n",
      "Step 1335: weight=0.6361, bias=0.3269\n",
      "Step 1336: weight=0.6362, bias=0.3268\n",
      "Step 1337: weight=0.6364, bias=0.3267\n",
      "Step 1338: weight=0.6366, bias=0.3267\n",
      "Step 1339: weight=0.6367, bias=0.3266\n",
      "Step 1340: weight=0.6369, bias=0.3265\n",
      "Step 1341: weight=0.6371, bias=0.3264\n",
      "Step 1342: weight=0.6372, bias=0.3264\n",
      "Step 1343: weight=0.6374, bias=0.3263\n",
      "Step 1344: weight=0.6376, bias=0.3262\n",
      "Step 1345: weight=0.6378, bias=0.3262\n",
      "Step 1346: weight=0.6379, bias=0.3261\n",
      "Step 1347: weight=0.6381, bias=0.3260\n",
      "Step 1348: weight=0.6383, bias=0.3259\n",
      "Step 1349: weight=0.6384, bias=0.3259\n",
      "Step 1350: weight=0.6386, bias=0.3258\n",
      "Step 1351: weight=0.6388, bias=0.3257\n",
      "Step 1352: weight=0.6390, bias=0.3257\n",
      "Step 1353: weight=0.6391, bias=0.3256\n",
      "Step 1354: weight=0.6393, bias=0.3255\n",
      "Step 1355: weight=0.6395, bias=0.3254\n",
      "Step 1356: weight=0.6396, bias=0.3254\n",
      "Step 1357: weight=0.6398, bias=0.3253\n",
      "Step 1358: weight=0.6400, bias=0.3252\n",
      "Step 1359: weight=0.6401, bias=0.3251\n",
      "Step 1360: weight=0.6403, bias=0.3251\n",
      "Step 1361: weight=0.6405, bias=0.3250\n",
      "Step 1362: weight=0.6407, bias=0.3249\n",
      "Step 1363: weight=0.6408, bias=0.3249\n",
      "Step 1364: weight=0.6410, bias=0.3248\n",
      "Step 1365: weight=0.6412, bias=0.3247\n",
      "Step 1366: weight=0.6413, bias=0.3246\n",
      "Step 1367: weight=0.6415, bias=0.3246\n",
      "Step 1368: weight=0.6417, bias=0.3245\n",
      "Step 1369: weight=0.6419, bias=0.3244\n",
      "Step 1370: weight=0.6420, bias=0.3244\n",
      "Step 1371: weight=0.6422, bias=0.3243\n",
      "Step 1372: weight=0.6424, bias=0.3242\n",
      "Step 1373: weight=0.6425, bias=0.3241\n",
      "Step 1374: weight=0.6427, bias=0.3241\n",
      "Step 1375: weight=0.6429, bias=0.3240\n",
      "Step 1376: weight=0.6431, bias=0.3239\n",
      "Step 1377: weight=0.6432, bias=0.3239\n",
      "Step 1378: weight=0.6434, bias=0.3238\n",
      "Step 1379: weight=0.6436, bias=0.3237\n",
      "Step 1380: weight=0.6437, bias=0.3236\n",
      "Step 1381: weight=0.6439, bias=0.3236\n",
      "Step 1382: weight=0.6441, bias=0.3235\n",
      "Step 1383: weight=0.6443, bias=0.3234\n",
      "Step 1384: weight=0.6444, bias=0.3234\n",
      "Step 1385: weight=0.6446, bias=0.3233\n",
      "Step 1386: weight=0.6448, bias=0.3232\n",
      "Step 1387: weight=0.6449, bias=0.3231\n",
      "Step 1388: weight=0.6451, bias=0.3231\n",
      "Step 1389: weight=0.6453, bias=0.3230\n",
      "Step 1390: weight=0.6454, bias=0.3229\n",
      "Step 1391: weight=0.6456, bias=0.3229\n",
      "Step 1392: weight=0.6458, bias=0.3228\n",
      "Step 1393: weight=0.6460, bias=0.3227\n",
      "Step 1394: weight=0.6461, bias=0.3226\n",
      "Step 1395: weight=0.6463, bias=0.3226\n",
      "Step 1396: weight=0.6465, bias=0.3225\n",
      "Step 1397: weight=0.6466, bias=0.3224\n",
      "Step 1398: weight=0.6468, bias=0.3224\n",
      "Step 1399: weight=0.6470, bias=0.3223\n",
      "Step 1400: weight=0.6472, bias=0.3222\n",
      "Step 1401: weight=0.6473, bias=0.3221\n",
      "Step 1402: weight=0.6475, bias=0.3221\n",
      "Step 1403: weight=0.6477, bias=0.3220\n",
      "Step 1404: weight=0.6478, bias=0.3219\n",
      "Step 1405: weight=0.6480, bias=0.3218\n",
      "Step 1406: weight=0.6482, bias=0.3218\n",
      "Step 1407: weight=0.6484, bias=0.3217\n",
      "Step 1408: weight=0.6485, bias=0.3216\n",
      "Step 1409: weight=0.6487, bias=0.3216\n",
      "Step 1410: weight=0.6489, bias=0.3215\n",
      "Step 1411: weight=0.6490, bias=0.3214\n",
      "Step 1412: weight=0.6492, bias=0.3213\n",
      "Step 1413: weight=0.6494, bias=0.3213\n",
      "Step 1414: weight=0.6496, bias=0.3212\n",
      "Step 1415: weight=0.6497, bias=0.3211\n",
      "Step 1416: weight=0.6499, bias=0.3211\n",
      "Step 1417: weight=0.6501, bias=0.3210\n",
      "Step 1418: weight=0.6502, bias=0.3209\n",
      "Step 1419: weight=0.6504, bias=0.3208\n",
      "Step 1420: weight=0.6506, bias=0.3208\n",
      "Step 1421: weight=0.6507, bias=0.3207\n",
      "Step 1422: weight=0.6509, bias=0.3206\n",
      "Step 1423: weight=0.6511, bias=0.3206\n",
      "Step 1424: weight=0.6513, bias=0.3205\n",
      "Step 1425: weight=0.6514, bias=0.3204\n",
      "Step 1426: weight=0.6516, bias=0.3203\n",
      "Step 1427: weight=0.6518, bias=0.3203\n",
      "Step 1428: weight=0.6519, bias=0.3202\n",
      "Step 1429: weight=0.6521, bias=0.3201\n",
      "Step 1430: weight=0.6523, bias=0.3201\n",
      "Step 1431: weight=0.6525, bias=0.3200\n",
      "Step 1432: weight=0.6526, bias=0.3199\n",
      "Step 1433: weight=0.6528, bias=0.3198\n",
      "Step 1434: weight=0.6530, bias=0.3198\n",
      "Step 1435: weight=0.6531, bias=0.3197\n",
      "Step 1436: weight=0.6533, bias=0.3196\n",
      "Step 1437: weight=0.6535, bias=0.3196\n",
      "Step 1438: weight=0.6536, bias=0.3195\n",
      "Step 1439: weight=0.6538, bias=0.3194\n",
      "Step 1440: weight=0.6540, bias=0.3193\n",
      "Step 1441: weight=0.6542, bias=0.3193\n",
      "Step 1442: weight=0.6543, bias=0.3192\n",
      "Step 1443: weight=0.6545, bias=0.3191\n",
      "Step 1444: weight=0.6547, bias=0.3190\n",
      "Step 1445: weight=0.6548, bias=0.3190\n",
      "Step 1446: weight=0.6550, bias=0.3189\n",
      "Step 1447: weight=0.6552, bias=0.3188\n",
      "Step 1448: weight=0.6554, bias=0.3188\n",
      "Step 1449: weight=0.6555, bias=0.3187\n",
      "Step 1450: weight=0.6557, bias=0.3186\n",
      "Step 1451: weight=0.6559, bias=0.3185\n",
      "Step 1452: weight=0.6560, bias=0.3185\n",
      "Step 1453: weight=0.6562, bias=0.3184\n",
      "Step 1454: weight=0.6564, bias=0.3183\n",
      "Step 1455: weight=0.6566, bias=0.3183\n",
      "Step 1456: weight=0.6567, bias=0.3182\n",
      "Step 1457: weight=0.6569, bias=0.3181\n",
      "Step 1458: weight=0.6571, bias=0.3180\n",
      "Step 1459: weight=0.6572, bias=0.3180\n",
      "Step 1460: weight=0.6574, bias=0.3179\n",
      "Step 1461: weight=0.6576, bias=0.3178\n",
      "Step 1462: weight=0.6578, bias=0.3178\n",
      "Step 1463: weight=0.6579, bias=0.3177\n",
      "Step 1464: weight=0.6581, bias=0.3176\n",
      "Step 1465: weight=0.6583, bias=0.3175\n",
      "Step 1466: weight=0.6584, bias=0.3175\n",
      "Step 1467: weight=0.6586, bias=0.3174\n",
      "Step 1468: weight=0.6588, bias=0.3173\n",
      "Step 1469: weight=0.6590, bias=0.3173\n",
      "Step 1470: weight=0.6591, bias=0.3172\n",
      "Step 1471: weight=0.6593, bias=0.3171\n",
      "Step 1472: weight=0.6595, bias=0.3170\n",
      "Step 1473: weight=0.6596, bias=0.3170\n",
      "Step 1474: weight=0.6598, bias=0.3169\n",
      "Step 1475: weight=0.6600, bias=0.3168\n",
      "Step 1476: weight=0.6602, bias=0.3168\n",
      "Step 1477: weight=0.6603, bias=0.3167\n",
      "Step 1478: weight=0.6605, bias=0.3166\n",
      "Step 1479: weight=0.6607, bias=0.3165\n",
      "Step 1480: weight=0.6608, bias=0.3165\n",
      "Step 1481: weight=0.6610, bias=0.3164\n",
      "Step 1482: weight=0.6612, bias=0.3163\n",
      "Step 1483: weight=0.6613, bias=0.3163\n",
      "Step 1484: weight=0.6615, bias=0.3162\n",
      "Step 1485: weight=0.6617, bias=0.3161\n",
      "Step 1486: weight=0.6618, bias=0.3160\n",
      "Step 1487: weight=0.6620, bias=0.3160\n",
      "Step 1488: weight=0.6622, bias=0.3159\n",
      "Step 1489: weight=0.6624, bias=0.3158\n",
      "Step 1490: weight=0.6625, bias=0.3157\n",
      "Step 1491: weight=0.6627, bias=0.3157\n",
      "Step 1492: weight=0.6629, bias=0.3156\n",
      "Step 1493: weight=0.6630, bias=0.3155\n",
      "Step 1494: weight=0.6632, bias=0.3155\n",
      "Step 1495: weight=0.6634, bias=0.3154\n",
      "Step 1496: weight=0.6636, bias=0.3153\n",
      "Step 1497: weight=0.6637, bias=0.3152\n",
      "Step 1498: weight=0.6639, bias=0.3152\n",
      "Step 1499: weight=0.6641, bias=0.3151\n",
      "Step 1500: weight=0.6642, bias=0.3150\n",
      "\n",
      "Target: weight=0.7, bias=0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Results (weight, bias):\")\n",
    "for i, (w, b) in enumerate(results):\n",
    "    print(f\"Step {i}: weight={w:.4f}, bias={b:.4f}\")\n",
    "\n",
    "print(f\"\\nTarget: weight={weight}, bias={bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6c296189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions now\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_0.forward(X_test)\n",
    "\n",
    "# Plot\n",
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d07868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
